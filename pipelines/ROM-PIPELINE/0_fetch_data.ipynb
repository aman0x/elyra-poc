{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbfcfb2-c1e8-4275-a8b1-b901233cb38e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing boto3...\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.38.21-py3-none-any.whl (139 kB)\n",
      "Collecting botocore<1.39.0,>=1.38.21\n",
      "  Downloading botocore-1.38.21-py3-none-any.whl (13.6 MB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.13.0,>=0.12.0\n",
      "  Downloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.21->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.38.21 botocore-1.38.21 jmespath-1.0.1 s3transfer-0.12.0\n",
      "Successfully installed boto3\n",
      "Dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package if needed.\"\"\"\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    print(f\"Successfully installed {package}\")\n",
    "\n",
    "# Install boto3 which is required for S3/MinIO operations\n",
    "install_package(\"boto3\")\n",
    "\n",
    "print(\"Dependencies installed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422a440c-abcf-45fe-bc14-223161c1b5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using temporary directory: /tmp/tmpg46ifiuc\n",
      "Connecting to MinIO...\n",
      "Testing MinIO connection to http://minio.minio-system.svc.cluster.local:9000\n",
      "Successfully connected to bucket rom-data\n",
      "Bucket rom-data exists and is accessible\n",
      "\n",
      "Looking for HDF5 files in rom-data/examples/cylinder/\n",
      "Found 1 objects in the directory\n",
      "Found 1 HDF5 files:\n",
      "  Downloading examples/cylinder/cylinder_data.hdf5 (9057238656 bytes) to /tmp/tmpg46ifiuc/cylinder_data.hdf5\n",
      "  ✓ Valid HDF5 file! Contents: ['coordinate_x', 'coordinate_y', 'density', 'momentum_x', 'momentum_y', 'pressure', 'time']\n",
      "Successfully downloaded 1 HDF5 files\n",
      "Created directory rom-pipeline/outputs/data/\n",
      "Uploading /tmp/tmpg46ifiuc/cylinder_data.hdf5 (9057238656 bytes) to rom-pipeline/outputs/data/cylinder_data.hdf5\n",
      "✓ Upload successful\n",
      "\n",
      "Inspecting and visualizing HDF5 files:\n",
      "\n",
      "Inspecting cylinder_data.hdf5:\n",
      "  Dataset: coordinate_x, Shape: (421, 600), Type: float64\n",
      "  Dataset: coordinate_y, Shape: (421, 600), Type: float64\n",
      "  Dataset: density, Shape: (421, 600, 1120), Type: float64\n",
      "  Dataset: momentum_x, Shape: (421, 600, 1120), Type: float64\n",
      "  Dataset: momentum_y, Shape: (421, 600, 1120), Type: float64\n",
      "  Dataset: pressure, Shape: (421, 600, 1120), Type: float64\n",
      "  Dataset: time, Shape: (1120,), Type: float64\n",
      "\n",
      "Data fetching and inspection completed!\n",
      "All data uploaded to rom-data/rom-pipeline/outputs/\n",
      "\n",
      "Verifying uploaded files:\n",
      "  - rom-pipeline/outputs/data/ (0 bytes)\n",
      "  - rom-pipeline/outputs/data/cylinder_data.hdf5 (9057238656 bytes)\n",
      "  - rom-pipeline/outputs/data_download_completed.txt (36 bytes)\n",
      "  - rom-pipeline/outputs/params.json (182 bytes)\n"
     ]
    }
   ],
   "source": [
    "# 0_fetch_data.ipynb\n",
    "# \n",
    "# This notebook fetches the Cylinder Flow Dataset from MinIO storage\n",
    "# and saves it to MinIO for further processing.\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "# Create a temporary local directory for processing\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Define MinIO parameters\n",
    "MINIO_BUCKET = 'rom-data'\n",
    "MINIO_RAW_PREFIX = 'examples/cylinder'\n",
    "MINIO_OUTPUT_PREFIX = 'rom-pipeline/outputs'\n",
    "\n",
    "# Log parameters for traceability\n",
    "params = {\n",
    "    \"dataset_name\": \"cylinder\",\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"minio_bucket\": MINIO_BUCKET,\n",
    "    \"minio_input_prefix\": MINIO_RAW_PREFIX,\n",
    "    \"minio_output_prefix\": MINIO_OUTPUT_PREFIX\n",
    "}\n",
    "\n",
    "# Connect to MinIO\n",
    "print(\"Connecting to MinIO...\")\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT', 'http://minio.minio-system.svc.cluster.local:9000')\n",
    "\n",
    "# Fix the endpoint URL if the protocol is missing\n",
    "if s3_endpoint and not s3_endpoint.startswith(('http://', 'https://')):\n",
    "    s3_endpoint = f\"http://{s3_endpoint}\"\n",
    "    print(f\"Adding http:// prefix to endpoint: {s3_endpoint}\")\n",
    "\n",
    "s3_access_key = os.environ.get('AWS_ACCESS_KEY_ID', 'minio')\n",
    "s3_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY', 'minio123')\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url=s3_endpoint,\n",
    "                  aws_access_key_id=s3_access_key,\n",
    "                  aws_secret_access_key=s3_secret_key,\n",
    "                  config=Config(signature_version='s3v4'))\n",
    "\n",
    "# Test MinIO connection\n",
    "try:\n",
    "    print(f\"Testing MinIO connection to {s3_endpoint}\")\n",
    "    s3.head_bucket(Bucket=MINIO_BUCKET)\n",
    "    print(f\"Successfully connected to bucket {MINIO_BUCKET}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to MinIO: {str(e)}\")\n",
    "    raise  # Re-raise to stop execution\n",
    "\n",
    "# Verify the bucket exists\n",
    "try:\n",
    "    s3.head_bucket(Bucket=MINIO_BUCKET)\n",
    "    print(f\"Bucket {MINIO_BUCKET} exists and is accessible\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        s3.create_bucket(Bucket=MINIO_BUCKET)\n",
    "        print(f\"Created bucket {MINIO_BUCKET}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Error accessing or creating bucket: {str(e2)}\")\n",
    "        raise\n",
    "\n",
    "# Look for all HDF5 files in the directory\n",
    "print(f\"\\nLooking for HDF5 files in {MINIO_BUCKET}/{MINIO_RAW_PREFIX}/\")\n",
    "have_data = False\n",
    "downloaded_files = []\n",
    "\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=MINIO_RAW_PREFIX)\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        print(f\"Found {len(response['Contents'])} objects in the directory\")\n",
    "        hdf5_files = [obj for obj in response['Contents'] \n",
    "                     if obj['Key'].endswith(('.h5', '.hdf5'))]\n",
    "        \n",
    "        if hdf5_files:\n",
    "            print(f\"Found {len(hdf5_files)} HDF5 files:\")\n",
    "            for obj in hdf5_files:\n",
    "                file_key = obj['Key']\n",
    "                filename = os.path.basename(file_key)\n",
    "                local_path = os.path.join(temp_dir, filename)\n",
    "                \n",
    "                print(f\"  Downloading {file_key} ({obj['Size']} bytes) to {local_path}\")\n",
    "                s3.download_file(MINIO_BUCKET, file_key, local_path)\n",
    "                \n",
    "                # Verify the file was downloaded and is a valid HDF5 file\n",
    "                if os.path.exists(local_path):\n",
    "                    try:\n",
    "                        with h5py.File(local_path, 'r') as f:\n",
    "                            print(f\"  ✓ Valid HDF5 file! Contents: {list(f.keys())}\")\n",
    "                            downloaded_files.append(local_path)\n",
    "                    except Exception as hdf_error:\n",
    "                        print(f\"  ✗ Error reading HDF5 file: {str(hdf_error)}\")\n",
    "            \n",
    "            if downloaded_files:\n",
    "                have_data = True\n",
    "                print(f\"Successfully downloaded {len(downloaded_files)} HDF5 files\")\n",
    "        else:\n",
    "            print(\"No HDF5 files found in the directory\")\n",
    "    else:\n",
    "        print(\"Directory is empty or does not exist\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error listing or downloading files: {str(e)}\")\n",
    "\n",
    "# If we still don't have data, create a sample dataset\n",
    "if not have_data:\n",
    "    print(\"\\nNo suitable data found, creating sample dataset\")\n",
    "    \n",
    "    # Create a simple 2D cylinder flow simulation for testing (very simplified)\n",
    "    n_snapshots = 50\n",
    "    n_x, n_y = 100, 50\n",
    "    \n",
    "    # Create the velocity fields\n",
    "    velocity = np.zeros((n_snapshots, n_x, n_y, 2))\n",
    "    \n",
    "    # Add a simple flow pattern (very simplistic cylinder wake)\n",
    "    x = np.linspace(0, 10, n_x)\n",
    "    y = np.linspace(-2.5, 2.5, n_y)\n",
    "    X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "    \n",
    "    # Generate time-varying flow\n",
    "    for t in range(n_snapshots):\n",
    "        # Base flow from left to right\n",
    "        u = np.ones_like(X) * 1.0\n",
    "        v = np.zeros_like(Y)\n",
    "        \n",
    "        # Add cylinder at x=2\n",
    "        cylinder_x, cylinder_y = 2, 0\n",
    "        cylinder_radius = 0.5\n",
    "        distance = np.sqrt((X - cylinder_x)**2 + (Y - cylinder_y)**2)\n",
    "        mask = distance < cylinder_radius\n",
    "        u[mask] = 0\n",
    "        v[mask] = 0\n",
    "        \n",
    "        # Add oscillating wake\n",
    "        wake_mask = (X > cylinder_x) & (distance > cylinder_radius)\n",
    "        phase = 0.2 * t\n",
    "        v[wake_mask] = 0.3 * np.sin(0.5 * (X[wake_mask] - cylinder_x) + phase) * np.exp(-(Y[wake_mask]**2) / 0.5)\n",
    "        \n",
    "        # Add some random noise\n",
    "        u += 0.02 * np.random.randn(*u.shape)\n",
    "        v += 0.02 * np.random.randn(*v.shape)\n",
    "        \n",
    "        # Store u and v components\n",
    "        velocity[t, :, :, 0] = u\n",
    "        velocity[t, :, :, 1] = v\n",
    "    \n",
    "    # Save to HDF5 file\n",
    "    sample_file = os.path.join(temp_dir, 'cylinder_flow.h5')\n",
    "    with h5py.File(sample_file, 'w') as f:\n",
    "        f.create_dataset('velocity', data=velocity)\n",
    "        f.create_dataset('x', data=x)\n",
    "        f.create_dataset('y', data=y)\n",
    "        f.create_dataset('time', data=np.linspace(0, 10, n_snapshots))\n",
    "    \n",
    "    print(f\"Created sample dataset at {sample_file}\")\n",
    "    \n",
    "    # Verify file was created\n",
    "    print(f\"Verifying sample file exists: {os.path.exists(sample_file)}\")\n",
    "    print(f\"Sample file size: {os.path.getsize(sample_file)} bytes\")\n",
    "    \n",
    "    # Check file content with h5py\n",
    "    with h5py.File(sample_file, 'r') as f:\n",
    "        print(f\"HDF5 file contents: {list(f.keys())}\")\n",
    "        print(f\"Velocity shape: {f['velocity'].shape}\")\n",
    "    \n",
    "    downloaded_files.append(sample_file)\n",
    "\n",
    "# CRITICAL: Ensure the data is copied to the output location\n",
    "# This section must always run, regardless of whether you found or created the data\n",
    "try:\n",
    "    # Create the output directory structure if needed\n",
    "    output_data_key = f\"{MINIO_OUTPUT_PREFIX}/data/\"\n",
    "    try:\n",
    "        s3.put_object(Bucket=MINIO_BUCKET, Key=output_data_key)\n",
    "        print(f\"Created directory {output_data_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {str(e)}\")\n",
    "    \n",
    "    # Find all HDF5 files in the temp directory\n",
    "    h5_files = [f for f in os.listdir(temp_dir) if f.endswith(('.h5', '.hdf5'))]\n",
    "    \n",
    "    if not h5_files:\n",
    "        raise FileNotFoundError(\"No HDF5 files found to upload!\")\n",
    "    \n",
    "    # Upload each file to the output location\n",
    "    uploaded_files = 0\n",
    "    for filename in h5_files:\n",
    "        source_path = os.path.join(temp_dir, filename)\n",
    "        target_key = f\"{MINIO_OUTPUT_PREFIX}/data/{filename}\"\n",
    "        \n",
    "        print(f\"Uploading {source_path} ({os.path.getsize(source_path)} bytes) to {target_key}\")\n",
    "        s3.upload_file(source_path, MINIO_BUCKET, target_key)\n",
    "        print(f\"✓ Upload successful\")\n",
    "        uploaded_files += 1\n",
    "    \n",
    "    if uploaded_files == 0:\n",
    "        raise Exception(\"No files were uploaded to MinIO - pipeline cannot continue\")\n",
    "    \n",
    "    # Create marker file to indicate success\n",
    "    marker_path = os.path.join(temp_dir, 'data_download_completed.txt')\n",
    "    with open(marker_path, 'w') as f:\n",
    "        f.write(\"Data download completed successfully\")\n",
    "    \n",
    "    s3.upload_file(\n",
    "        marker_path,\n",
    "        MINIO_BUCKET, \n",
    "        f\"{MINIO_OUTPUT_PREFIX}/data_download_completed.txt\"\n",
    "    )\n",
    "    \n",
    "    # Upload parameters file to MinIO\n",
    "    params_path = os.path.join(temp_dir, 'params.json')\n",
    "    with open(params_path, 'w') as f:\n",
    "        json.dump(params, f)\n",
    "    \n",
    "    s3.upload_file(\n",
    "        params_path,\n",
    "        MINIO_BUCKET, \n",
    "        f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR during data upload: {str(e)}\")\n",
    "    raise  # Re-raise to stop execution\n",
    "\n",
    "# Inspect and visualize HDF5 files\n",
    "if downloaded_files:\n",
    "    print(\"\\nInspecting and visualizing HDF5 files:\")\n",
    "    \n",
    "    for file_path in downloaded_files:\n",
    "        h5_file = os.path.basename(file_path)\n",
    "        print(f\"\\nInspecting {h5_file}:\")\n",
    "        \n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                # Function to recursively print HDF5 group structure\n",
    "                def print_group(name, obj):\n",
    "                    if isinstance(obj, h5py.Dataset):\n",
    "                        print(f\"  Dataset: {name}, Shape: {obj.shape}, Type: {obj.dtype}\")\n",
    "                    elif isinstance(obj, h5py.Group):\n",
    "                        print(f\"  Group: {name}\")\n",
    "                \n",
    "                # Traverse the file structure\n",
    "                f.visititems(print_group)\n",
    "                \n",
    "                # If the file contains velocity data, plot a sample\n",
    "                if 'velocity' in f:\n",
    "                    velocity = f['velocity']\n",
    "                    if len(velocity.shape) >= 3:  # Time, X, Y, (possibly components)\n",
    "                        # Plot the first velocity snapshot\n",
    "                        plt.figure(figsize=(10, 6))\n",
    "                        \n",
    "                        # Handle different velocity field formats\n",
    "                        if len(velocity.shape) == 3:  # Time, X, Y\n",
    "                            plt.imshow(velocity[0], cmap='viridis')\n",
    "                            plt.title('First velocity snapshot (scalar)')\n",
    "                        elif len(velocity.shape) == 4:  # Time, X, Y, Components\n",
    "                            # Plot magnitude of velocity\n",
    "                            v_mag = np.sqrt(velocity[0,:,:,0]**2 + velocity[0,:,:,1]**2)\n",
    "                            plt.imshow(v_mag, cmap='viridis')\n",
    "                            plt.title('First velocity snapshot (magnitude)')\n",
    "                        \n",
    "                        plt.colorbar(label='Velocity')\n",
    "                        plt.xlabel('X')\n",
    "                        plt.ylabel('Y')\n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        # Save the plot to a file\n",
    "                        img_path = os.path.join(temp_dir, 'velocity_sample.png')\n",
    "                        plt.savefig(img_path)\n",
    "                        \n",
    "                        # Upload the visualization to MinIO\n",
    "                        s3.upload_file(\n",
    "                            img_path,\n",
    "                            MINIO_BUCKET, \n",
    "                            f\"{MINIO_OUTPUT_PREFIX}/visualizations/velocity_sample.png\"\n",
    "                        )\n",
    "                        \n",
    "                        # Save the first few velocity snapshots for inspection\n",
    "                        npy_path = os.path.join(temp_dir, 'velocity_samples.npy')\n",
    "                        np.save(npy_path, velocity[:5] if len(velocity) >= 5 else velocity[:])\n",
    "                        \n",
    "                        print(f\"  Sample visualization uploaded to {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/visualizations/velocity_sample.png\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error inspecting {h5_file}: {str(e)}\")\n",
    "\n",
    "print(\"\\nData fetching and inspection completed!\")\n",
    "print(f\"All data uploaded to {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/\")\n",
    "\n",
    "# List objects in the bucket to verify uploads\n",
    "print(\"\\nVerifying uploaded files:\")\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=f\"{MINIO_OUTPUT_PREFIX}/\")\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "else:\n",
    "    print(\"No objects found in output location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d3253-da25-47ba-ae54-7110c41afeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
