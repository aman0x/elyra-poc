{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0046bf-ad60-42d6-b99a-2985374581bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing boto3...\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.9/site-packages (1.38.21)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.21 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.38.21)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from boto3) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (1.26.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.21->boto3) (1.16.0)\n",
      "Successfully installed boto3\n",
      "Installing h5py...\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.9/site-packages (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from h5py) (1.24.2)\n",
      "Successfully installed h5py\n",
      "Installing matplotlib...\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.7.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (5.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Successfully installed matplotlib\n",
      "Installing psutil...\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (5.9.4)\n",
      "Successfully installed psutil\n",
      "Installing numpy...\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.24.2)\n",
      "Successfully installed numpy\n",
      "Dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package, version=None):\n",
    "    \"\"\"Install a package with specific version if needed.\"\"\"\n",
    "    if version:\n",
    "        package_with_version = f\"{package}=={version}\"\n",
    "    else:\n",
    "        package_with_version = package\n",
    "    \n",
    "    print(f\"Installing {package_with_version}...\")\n",
    "    # Allow all packages to install their dependencies\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_with_version])\n",
    "    print(f\"Successfully installed {package_with_version}\")\n",
    "\n",
    "# Only install non-standard library packages\n",
    "install_package(\"boto3\")       # For S3/MinIO operations (will install botocore)\n",
    "install_package(\"h5py\")        # For HDF5 file operations\n",
    "install_package(\"matplotlib\")  # For plotting\n",
    "install_package(\"psutil\")      # For memory tracking\n",
    "install_package(\"numpy\")       # For numerical operations\n",
    "\n",
    "print(\"Dependencies installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a772d-c79e-49a5-9e77-0fdea33c2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2_rom_modeling.ipynb\n",
    "#\n",
    "# This notebook preprocesses the Cylinder Flow Dataset with memory optimizations:\n",
    "# - Loads the data from MinIO\n",
    "# - Extracts velocity fields\n",
    "# - Creates the snapshot matrix\n",
    "# - Performs mean subtraction\n",
    "# - Normalizes the data if needed\n",
    "# - Uploads processed data back to MinIO\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import tempfile\n",
    "import io\n",
    "import gc  # For garbage collection\n",
    "import psutil  # For memory tracking\n",
    "\n",
    "# Memory management flags\n",
    "MEMORY_EFFICIENT = True\n",
    "CHUNK_SIZE = 10  # Process this many snapshots at a time\n",
    "ENABLE_MEMORY_TRACKING = True\n",
    "DOWNSAMPLE = False\n",
    "DOWNSAMPLE_FACTOR = 2  # Only use every Nth point in grid\n",
    "\n",
    "# Thread control to limit memory usage\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"2\"\n",
    "\n",
    "# Function to track memory usage\n",
    "def print_memory_usage(label=\"\"):\n",
    "    if ENABLE_MEMORY_TRACKING:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"Memory usage {label}: {memory_mb:.1f} MB\")\n",
    "\n",
    "print_memory_usage(\"at start\")\n",
    "\n",
    "# Function to convert NumPy types to Python native types for JSON serialization\n",
    "def json_serialize(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, tuple) and hasattr(obj, '_asdict'):\n",
    "        # Handle named tuples\n",
    "        return obj._asdict()\n",
    "    elif isinstance(obj, tuple):\n",
    "        return list(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_dict_to_json(data_dict, filepath):\n",
    "    \"\"\"Save dictionary to JSON with NumPy value conversion\"\"\"\n",
    "    # Convert data to JSON-serializable types\n",
    "    serializable_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        serializable_dict[key] = json_serialize(value)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(serializable_dict, f, indent=2)\n",
    "\n",
    "# Create a temporary local directory for processing\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Connect to MinIO\n",
    "print(\"Connecting to MinIO...\")\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT', 'http://minio:9000' )\n",
    "\n",
    "# Fix the endpoint URL if the protocol is missing\n",
    "if s3_endpoint and not s3_endpoint.startswith(('http://', 'https://' )):\n",
    "    s3_endpoint = f\"http://{s3_endpoint}\"\n",
    "    print(f\"Adding http:// prefix to endpoint: {s3_endpoint}\" )\n",
    "\n",
    "s3_access_key = os.environ.get('AWS_ACCESS_KEY_ID', 'minioadmin')\n",
    "s3_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY', 'minioadmin')\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url=s3_endpoint,\n",
    "                  aws_access_key_id=s3_access_key,\n",
    "                  aws_secret_access_key=s3_secret_key,\n",
    "                  config=Config(signature_version='s3v4'))\n",
    "\n",
    "# Load parameters from previous step\n",
    "MINIO_BUCKET = 'rom-data'\n",
    "MINIO_OUTPUT_PREFIX = 'rom-pipeline/outputs'\n",
    "\n",
    "# Download parameters file from MinIO\n",
    "try:\n",
    "    params_key = f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "    params_path = os.path.join(temp_dir, 'params.json')\n",
    "    s3.download_file(MINIO_BUCKET, params_key, params_path)\n",
    "    \n",
    "    with open(params_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "        \n",
    "    print(f\"Loaded parameters successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading parameters, using defaults: {str(e)}\")\n",
    "    params = {\n",
    "        \"dataset_name\": \"cylinder\",\n",
    "        \"minio_bucket\": MINIO_BUCKET,\n",
    "        \"minio_output_prefix\": MINIO_OUTPUT_PREFIX\n",
    "    }\n",
    "\n",
    "# Update parameters for this step\n",
    "params.update({\n",
    "    \"preprocessing_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"preprocessing_options\": {\n",
    "        \"mean_subtraction\": True,\n",
    "        \"normalization\": True,\n",
    "        \"memory_efficient\": MEMORY_EFFICIENT,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"downsample\": DOWNSAMPLE,\n",
    "        \"downsample_factor\": DOWNSAMPLE_FACTOR if DOWNSAMPLE else None\n",
    "    }\n",
    "})\n",
    "\n",
    "# Verify the previous step completed\n",
    "try:\n",
    "    marker_key = f\"{MINIO_OUTPUT_PREFIX}/data_download_completed.txt\"\n",
    "    s3.head_object(Bucket=MINIO_BUCKET, Key=marker_key)\n",
    "    print(\"Previous step (data fetching) completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Previous step completion marker not found: {str(e)}\")\n",
    "    print(\"Continuing anyway...\")\n",
    "\n",
    "# List objects in the data directory to check what's available\n",
    "print(\"\\nListing files in MinIO data directory:\")\n",
    "data_prefix = f\"{MINIO_OUTPUT_PREFIX}/data/\"\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=data_prefix)\n",
    "\n",
    "if 'Contents' not in response or len(response.get('Contents', [])) == 0:\n",
    "    print(f\"Warning: No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "    print(\"Please ensure that data files have been uploaded.\")\n",
    "    raise FileNotFoundError(f\"No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "\n",
    "# Download data files from MinIO\n",
    "print(\"\\nDownloading data files from MinIO:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    if obj['Key'].endswith(('.h5', '.hdf5')):\n",
    "        filename = os.path.basename(obj['Key'])\n",
    "        local_path = os.path.join(temp_dir, filename)\n",
    "        \n",
    "        print(f\"  Downloading {obj['Key']} to {local_path}\")\n",
    "        s3.download_file(MINIO_BUCKET, obj['Key'], local_path)\n",
    "\n",
    "# Find HDF5 files\n",
    "h5_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) \n",
    "            if f.endswith('.h5') or f.endswith('.hdf5')]\n",
    "\n",
    "if not h5_files:\n",
    "    raise FileNotFoundError(f\"No HDF5 files found in downloaded data\")\n",
    "\n",
    "print(f\"Found {len(h5_files)} HDF5 files: {[os.path.basename(f) for f in h5_files]}\")\n",
    "\n",
    "# Load velocity data from the first file\n",
    "h5_file = h5_files[0]\n",
    "print(f\"Loading data from {os.path.basename(h5_file)}\")\n",
    "\n",
    "# Load the data or just get its shape\n",
    "data = None  # We'll load data in chunks if memory efficient mode is on\n",
    "shape = None\n",
    "velocity_key = None\n",
    "\n",
    "try:\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # First, explore the HDF5 file structure to find the velocity field\n",
    "        print(\"Exploring HDF5 file structure:\")\n",
    "        print(\"Top-level groups/datasets:\")\n",
    "        for key in f.keys():\n",
    "            if isinstance(f[key], h5py.Group):\n",
    "                print(f\"  Group: {key} (contains: {list(f[key].keys())})\")\n",
    "            elif isinstance(f[key], h5py.Dataset):\n",
    "                print(f\"  Dataset: {key} (shape: {f[key].shape}, dtype: {f[key].dtype})\")\n",
    "        \n",
    "        # Define possible velocity field names to check\n",
    "        velocity_keys = ['velocity', 'u', 'v', 'vel', 'flow', 'flowfield', 'vector']\n",
    "        \n",
    "        # Search for velocity data\n",
    "        velocity_key = None\n",
    "        \n",
    "        # Step 1: Check direct keys in root level\n",
    "        for key in velocity_keys:\n",
    "            if key in f:\n",
    "                velocity_key = key\n",
    "                print(f\"Found direct velocity key: {key}\")\n",
    "                break\n",
    "        \n",
    "        # Step 2: Look for keys containing velocity terms\n",
    "        if velocity_key is None:\n",
    "            for key in f.keys():\n",
    "                if any(vk in key.lower() for vk in velocity_keys):\n",
    "                    velocity_key = key\n",
    "                    print(f\"Found key containing velocity term: {key}\")\n",
    "                    break\n",
    "        \n",
    "        # Step 3: Search in groups\n",
    "        if velocity_key is None:\n",
    "            for key in f.keys():\n",
    "                if isinstance(f[key], h5py.Group):\n",
    "                    for subkey in f[key].keys():\n",
    "                        if any(vk == subkey.lower() or vk in subkey.lower() for vk in velocity_keys):\n",
    "                            velocity_key = f\"{key}/{subkey}\"\n",
    "                            print(f\"Found velocity data in group: {velocity_key}\")\n",
    "                            break\n",
    "        \n",
    "        # Step 4: Look for large datasets (likely to be the flow field)\n",
    "        if velocity_key is None:\n",
    "            largest_dataset = None\n",
    "            largest_size = 0\n",
    "            \n",
    "            def find_largest_dataset(name, obj):\n",
    "                global largest_dataset, largest_size\n",
    "                if isinstance(obj, h5py.Dataset):\n",
    "                    # Skip small datasets (likely metadata)\n",
    "                    if obj.size > 1000 and obj.size > largest_size:\n",
    "                        largest_size = obj.size\n",
    "                        largest_dataset = name\n",
    "            \n",
    "            # Traverse the entire file\n",
    "            f.visititems(find_largest_dataset)\n",
    "            \n",
    "            if largest_dataset:\n",
    "                velocity_key = largest_dataset\n",
    "                print(f\"Selected largest dataset as velocity data: {velocity_key} (size: {largest_size})\")\n",
    "        \n",
    "        if velocity_key is None:\n",
    "            raise KeyError(\"Could not find velocity data in the file\")\n",
    "            \n",
    "        print(f\"Loading velocity data from '{velocity_key}'\")\n",
    "        \n",
    "        # Just get the shape if memory efficient\n",
    "        if MEMORY_EFFICIENT:\n",
    "            shape = f[velocity_key].shape\n",
    "            print(f\"Data shape: {shape} (memory efficient mode - not loading full data)\")\n",
    "        else:\n",
    "            data = f[velocity_key][:]\n",
    "            shape = data.shape\n",
    "            print(f\"Data shape: {shape}\")\n",
    "        \n",
    "        # Load any coordinates or metadata if available\n",
    "        x_coords = None\n",
    "        y_coords = None\n",
    "        time_data = None\n",
    "        \n",
    "        # Look for common coordinate names\n",
    "        for coord_name in ['x', 'y', 'z', 'X', 'Y', 'Z', 'coord', 'coords', 'coordinates', 'grid', 'time', 't']:\n",
    "            if coord_name in f:\n",
    "                if coord_name.lower() in ['x', 'X']:\n",
    "                    x_coords = f[coord_name][:]\n",
    "                    print(f\"Found x coordinates: shape {x_coords.shape}\")\n",
    "                elif coord_name.lower() in ['y', 'Y']:\n",
    "                    y_coords = f[coord_name][:]\n",
    "                    print(f\"Found y coordinates: shape {y_coords.shape}\")\n",
    "                elif coord_name.lower() in ['time', 't']:\n",
    "                    time_data = f[coord_name][:]\n",
    "                    print(f\"Found time data: shape {time_data.shape}\")\n",
    "                else:\n",
    "                    print(f\"Found other coordinate data '{coord_name}': shape {f[coord_name].shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print_memory_usage(\"after data exploration\")\n",
    "\n",
    "# Determine data dimensions\n",
    "if len(shape) == 2:  # (time, points) or (points, time)\n",
    "    if shape[0] < shape[1]:  # Likely (time, points)\n",
    "        n_snapshots = shape[0]\n",
    "        n_points = shape[1]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (points, time)\n",
    "        n_snapshots = shape[1]\n",
    "        n_points = shape[0]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "        \n",
    "elif len(shape) == 3:  # (time, x, y) or (time, points, dims)\n",
    "    if shape[1] > shape[2]:  # Likely (time, points, dims)\n",
    "        n_snapshots = shape[0]\n",
    "        n_points = shape[1]\n",
    "        n_dims = shape[2]\n",
    "        structured_grid = False\n",
    "        print(f\"Detected (time, points, dimensions) format: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (time, x, y) for a scalar field\n",
    "        n_snapshots = shape[0]\n",
    "        grid_shape = (shape[1], shape[2])\n",
    "        n_points = grid_shape[0] * grid_shape[1]\n",
    "        n_dims = 1\n",
    "        structured_grid = True\n",
    "        print(f\"Detected structured scalar data: {n_snapshots} snapshots, grid shape {grid_shape}\")\n",
    "    # REMOVED THE PROBLEMATIC SECOND ELSE STATEMENT HERE\n",
    "\n",
    "elif len(shape) == 4:  # Likely (time, x, y, components) \n",
    "    if shape[0] < shape[1] and shape[0] < shape[2]:  # Typical for time series\n",
    "        n_snapshots = shape[0]\n",
    "        grid_shape = (shape[1], shape[2])\n",
    "        n_points = np.prod(grid_shape)\n",
    "        n_dims = shape[3]\n",
    "        structured_grid = True\n",
    "        print(f\"Detected (time, x, y, components) format: {n_snapshots} snapshots, grid shape {grid_shape}, {n_dims} dimensions\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not determine data structure from shape {shape}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported data shape: {shape}\")\n",
    "\n",
    "# Create snapshot matrix and process mean/fluctuations\n",
    "if MEMORY_EFFICIENT and data is None:\n",
    "    print(f\"Processing data in chunks of {CHUNK_SIZE} snapshots to save memory\")\n",
    "    # Initialize arrays for the final results\n",
    "    snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "    mean_flow = np.zeros((n_points * n_dims, 1))\n",
    "    \n",
    "    # Process data in chunks\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # We'll compute the mean in a streaming fashion\n",
    "        chunk_means = []\n",
    "        chunk_weights = []\n",
    "        \n",
    "        # First pass: load data in chunks and compute chunk means\n",
    "        for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "            chunk_size = chunk_end - chunk_start\n",
    "            print(f\"Processing chunk {chunk_start+1}-{chunk_end} of {n_snapshots}\")\n",
    "            \n",
    "            # Load chunk\n",
    "            chunk_data = f[velocity_key][chunk_start:chunk_end]\n",
    "            \n",
    "            # Reshape chunk based on data structure\n",
    "            if structured_grid:\n",
    "                if n_dims == 1:\n",
    "                    # For scalar field\n",
    "                    chunk_reshaped = np.zeros((n_points, chunk_size))\n",
    "                    for i in range(chunk_size):\n",
    "                        chunk_reshaped[:, i] = chunk_data[i].reshape(n_points)\n",
    "                else:\n",
    "                    # For vector field\n",
    "                    chunk_reshaped = np.zeros((n_points * n_dims, chunk_size))\n",
    "                    for i in range(chunk_size):\n",
    "                        reshaped = chunk_data[i].reshape(n_points, n_dims)\n",
    "                        chunk_reshaped[:, i] = reshaped.flatten()\n",
    "            else:\n",
    "                # Already in the right format for unstructured grid\n",
    "                if n_dims == 1:\n",
    "                    if chunk_data.shape[0] < chunk_data.shape[1]:\n",
    "                        chunk_reshaped = chunk_data.T  # Transpose to get (n_points, chunk_size)\n",
    "                    else:\n",
    "                        chunk_reshaped = chunk_data  # Already (n_points, chunk_size)\n",
    "                else:\n",
    "                    chunk_reshaped = np.zeros((n_points * n_dims, chunk_size))\n",
    "                    for i in range(chunk_size):\n",
    "                        chunk_reshaped[:, i] = chunk_data[i].flatten()\n",
    "            \n",
    "            # Store in snapshot matrix\n",
    "            snapshot_matrix[:, chunk_start:chunk_end] = chunk_reshaped\n",
    "            \n",
    "            # Compute chunk mean\n",
    "            chunk_mean = np.mean(chunk_reshaped, axis=1, keepdims=True)\n",
    "            chunk_means.append(chunk_mean)\n",
    "            chunk_weights.append(chunk_size)\n",
    "            \n",
    "            # Free memory\n",
    "            del chunk_data, chunk_reshaped\n",
    "            gc.collect()\n",
    "            print_memory_usage(f\"after processing chunk {chunk_start+1}-{chunk_end}\")\n",
    "        \n",
    "        # Compute weighted mean of all chunks\n",
    "        total_weight = sum(chunk_weights)\n",
    "        for i, (mean, weight) in enumerate(zip(chunk_means, chunk_weights)):\n",
    "            mean_flow += (mean * weight / total_weight)\n",
    "        \n",
    "        print(f\"Computed mean flow with shape {mean_flow.shape}\")\n",
    "        \n",
    "        # Second pass: subtract mean and normalize if needed\n",
    "        if params[\"preprocessing_options\"][\"mean_subtraction\"]:\n",
    "            print(\"Applying mean subtraction...\")\n",
    "            for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "                chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "                print(f\"Subtracting mean from chunk {chunk_start+1}-{chunk_end}\")\n",
    "                \n",
    "                # Subtract mean\n",
    "                snapshot_matrix[:, chunk_start:chunk_end] -= mean_flow\n",
    "                \n",
    "                print_memory_usage(f\"after mean subtraction for chunk {chunk_start+1}-{chunk_end}\")\n",
    "            \n",
    "            print(\"Mean subtraction completed\")\n",
    "        else:\n",
    "            print(\"Mean subtraction skipped\")\n",
    "        \n",
    "        # Normalize if needed\n",
    "        if params[\"preprocessing_options\"][\"normalization\"]:\n",
    "            print(\"Applying normalization...\")\n",
    "            # Compute Frobenius norm in chunks to save memory\n",
    "            frob_norm_squared = 0\n",
    "            for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "                chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "                chunk = snapshot_matrix[:, chunk_start:chunk_end]\n",
    "                frob_norm_squared += np.sum(chunk**2)\n",
    "            \n",
    "            frob_norm = np.sqrt(frob_norm_squared)\n",
    "            print(f\"Computed Frobenius norm: {frob_norm:.4f}\")\n",
    "            \n",
    "            # Normalize\n",
    "            for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "                chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "                print(f\"Normalizing chunk {chunk_start+1}-{chunk_end}\")\n",
    "                snapshot_matrix[:, chunk_start:chunk_end] /= frob_norm\n",
    "            \n",
    "            # Save the normalization factor for later use\n",
    "            params[\"preprocessing_options\"][\"normalization_factor\"] = float(frob_norm)\n",
    "            print(\"Normalization completed\")\n",
    "        else:\n",
    "            print(\"Normalization skipped\")\n",
    "else:\n",
    "    # Process the full data at once (non-memory-efficient mode)\n",
    "    print(\"Processing all data at once\")\n",
    "    \n",
    "    # Create snapshot matrix - reshape data into a 2D matrix\n",
    "    # For POD, we need a matrix of shape (n_points*n_dims, n_snapshots)\n",
    "    if structured_grid:\n",
    "        if n_dims == 1:\n",
    "            # For scalar field\n",
    "            snapshot_matrix = np.zeros((n_points, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                snapshot_matrix[:, i] = data[i].reshape(n_points)\n",
    "        else:\n",
    "            # For vector field\n",
    "            snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                # Reshape and stack components\n",
    "                reshaped = data[i].reshape(n_points, n_dims)\n",
    "                snapshot_matrix[:, i] = reshaped.flatten()\n",
    "    else:\n",
    "        # Already in the right format for unstructured grid\n",
    "        if n_dims == 1:\n",
    "            if data.shape[0] < data.shape[1]:\n",
    "                snapshot_matrix = data.T  # Transpose to get (n_points, n_snapshots)\n",
    "            else:\n",
    "                snapshot_matrix = data  # Already (n_points, n_snapshots)\n",
    "        else:\n",
    "            snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                snapshot_matrix[:, i] = data[i].flatten()\n",
    "    \n",
    "    print(f\"Created snapshot matrix with shape: {snapshot_matrix.shape}\")\n",
    "    \n",
    "    # Compute mean flow\n",
    "    mean_flow = np.mean(snapshot_matrix, axis=1, keepdims=True)\n",
    "    print(f\"Mean flow shape: {mean_flow.shape}\")\n",
    "    \n",
    "    # Subtract mean (centering the data)\n",
    "    if params[\"preprocessing_options\"][\"mean_subtraction\"]:\n",
    "        snapshot_matrix -= mean_flow\n",
    "        print(\"Mean subtraction applied\")\n",
    "    else:\n",
    "        print(\"Mean subtraction skipped\")\n",
    "    \n",
    "    # Normalize the data if needed\n",
    "    if params[\"preprocessing_options\"][\"normalization\"]:\n",
    "        # Compute the Frobenius norm of the fluctuation matrix\n",
    "        frob_norm = np.linalg.norm(snapshot_matrix)\n",
    "        # Normalize\n",
    "        snapshot_matrix = snapshot_matrix / frob_norm\n",
    "        print(f\"Normalization applied (Frobenius norm: {frob_norm:.4f})\")\n",
    "        \n",
    "        # Save the normalization factor for later use\n",
    "        params[\"preprocessing_options\"][\"normalization_factor\"] = float(frob_norm)\n",
    "    else:\n",
    "        print(\"Normalization skipped\")\n",
    "\n",
    "print_memory_usage(\"after preprocessing\")\n",
    "\n",
    "# Save metadata about the preprocessing\n",
    "metadata = {\n",
    "    \"n_snapshots\": n_snapshots,\n",
    "    \"n_points\": n_points,\n",
    "    \"n_dims\": n_dims,\n",
    "    \"structured_grid\": structured_grid\n",
    "}\n",
    "\n",
    "if structured_grid:\n",
    "    metadata[\"grid_shape\"] = grid_shape\n",
    "\n",
    "# Update parameters with metadata\n",
    "params.update(metadata)\n",
    "\n",
    "# Save metadata to local file\n",
    "metadata_path = os.path.join(temp_dir, 'metadata.json')\n",
    "save_dict_to_json(metadata, metadata_path)\n",
    "\n",
    "# Save mean flow to local file\n",
    "mean_flow_path = os.path.join(temp_dir, 'mean_flow.npy')\n",
    "np.save(mean_flow_path, mean_flow)\n",
    "\n",
    "# Save snapshot matrix to local file\n",
    "snapshot_matrix_path = os.path.join(temp_dir, 'snapshot_matrix.npy')\n",
    "np.save(snapshot_matrix_path, snapshot_matrix)\n",
    "\n",
    "# Upload files to MinIO\n",
    "print(\"\\nUploading preprocessed data to MinIO:\")\n",
    "preprocessed_prefix = f\"{MINIO_OUTPUT_PREFIX}/preprocessed/\"\n",
    "\n",
    "# Upload metadata\n",
    "metadata_key = f\"{preprocessed_prefix}metadata.json\"\n",
    "print(f\"Uploading metadata to {metadata_key}\")\n",
    "s3.upload_file(metadata_path, MINIO_BUCKET, metadata_key)\n",
    "\n",
    "# Upload mean flow\n",
    "mean_flow_key = f\"{preprocessed_prefix}mean_flow.npy\"\n",
    "print(f\"Uploading mean flow to {mean_flow_key}\")\n",
    "s3.upload_file(mean_flow_path, MINIO_BUCKET, mean_flow_key)\n",
    "\n",
    "# Upload snapshot matrix\n",
    "snapshot_matrix_key = f\"{preprocessed_prefix}snapshot_matrix.npy\"\n",
    "print(f\"Uploading snapshot matrix to {snapshot_matrix_key}\")\n",
    "s3.upload_file(snapshot_matrix_path, MINIO_BUCKET, snapshot_matrix_key)\n",
    "\n",
    "# Update and upload parameters\n",
    "params_path = os.path.join(temp_dir, 'params.json')\n",
    "save_dict_to_json(params, params_path)\n",
    "params_key = f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "print(f\"Uploading updated parameters to {params_key}\")\n",
    "s3.upload_file(params_path, MINIO_BUCKET, params_key)\n",
    "\n",
    "# Create a marker file to indicate this step is complete\n",
    "marker_path = os.path.join(temp_dir, 'preprocessing_completed.txt')\n",
    "with open(marker_path, 'w') as f:\n",
    "    f.write(f\"Preprocessing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "marker_key = f\"{MINIO_OUTPUT_PREFIX}/preprocessing_completed.txt\"\n",
    "s3.upload_file(marker_path, MINIO_BUCKET, marker_key)\n",
    "\n",
    "print(\"\\nPreprocessing completed successfully!\")\n",
    "print(f\"Processed {n_snapshots} snapshots with {n_points} spatial points and {n_dims} dimensions\")\n",
    "print(f\"All results uploaded to MinIO bucket: {MINIO_BUCKET}, prefix: {preprocessed_prefix}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
