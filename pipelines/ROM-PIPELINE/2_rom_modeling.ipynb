{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a772d-c79e-49a5-9e77-0fdea33c2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1_preprocess_data.ipynb\n",
    "#\n",
    "# This notebook preprocesses the Cylinder Flow Dataset with memory optimizations:\n",
    "# - Loads the data from MinIO\n",
    "# - Extracts velocity fields\n",
    "# - Creates the snapshot matrix\n",
    "# - Performs mean subtraction\n",
    "# - Normalizes the data if needed\n",
    "# - Uploads processed data back to MinIO\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import tempfile\n",
    "import io\n",
    "import gc  # For garbage collection\n",
    "import psutil  # For memory tracking\n",
    "\n",
    "# Memory management flags\n",
    "MEMORY_EFFICIENT = True\n",
    "CHUNK_SIZE = 10  # Process this many snapshots at a time\n",
    "ENABLE_MEMORY_TRACKING = True\n",
    "DOWNSAMPLE = False\n",
    "DOWNSAMPLE_FACTOR = 2  # Only use every Nth point in grid\n",
    "\n",
    "# Thread control to limit memory usage\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"2\"\n",
    "\n",
    "# Function to track memory usage\n",
    "def print_memory_usage(label=\"\"):\n",
    "    if ENABLE_MEMORY_TRACKING:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"Memory usage {label}: {memory_mb:.1f} MB\")\n",
    "\n",
    "print_memory_usage(\"at start\")\n",
    "\n",
    "# Function to convert NumPy types to Python native types for JSON serialization\n",
    "def json_serialize(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, tuple) and hasattr(obj, '_asdict'):\n",
    "        # Handle named tuples\n",
    "        return obj._asdict()\n",
    "    elif isinstance(obj, tuple):\n",
    "        return list(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_dict_to_json(data_dict, filepath):\n",
    "    \"\"\"Save dictionary to JSON with NumPy value conversion\"\"\"\n",
    "    # Convert data to JSON-serializable types\n",
    "    serializable_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        serializable_dict[key] = json_serialize(value)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(serializable_dict, f, indent=2)\n",
    "\n",
    "# Create a temporary local directory for processing\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Connect to MinIO\n",
    "print(\"Connecting to MinIO...\")\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT', 'http://minio:9000')\n",
    "\n",
    "# Fix the endpoint URL if the protocol is missing\n",
    "if s3_endpoint and not s3_endpoint.startswith(('http://', 'https://')):\n",
    "    s3_endpoint = f\"http://{s3_endpoint}\"\n",
    "    print(f\"Adding http:// prefix to endpoint: {s3_endpoint}\")\n",
    "\n",
    "s3_access_key = os.environ.get('AWS_ACCESS_KEY_ID', 'minioadmin')\n",
    "s3_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY', 'minioadmin')\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url=s3_endpoint,\n",
    "                  aws_access_key_id=s3_access_key,\n",
    "                  aws_secret_access_key=s3_secret_key,\n",
    "                  config=Config(signature_version='s3v4'))\n",
    "\n",
    "# Load parameters from previous step\n",
    "MINIO_BUCKET = 'rom-data'\n",
    "MINIO_OUTPUT_PREFIX = 'rom-pipeline/outputs'\n",
    "\n",
    "# Download parameters file from MinIO\n",
    "try:\n",
    "    params_key = f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "    params_path = os.path.join(temp_dir, 'params.json')\n",
    "    s3.download_file(MINIO_BUCKET, params_key, params_path)\n",
    "    \n",
    "    with open(params_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "        \n",
    "    print(f\"Loaded parameters successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading parameters, using defaults: {str(e)}\")\n",
    "    params = {\n",
    "        \"dataset_name\": \"cylinder\",\n",
    "        \"minio_bucket\": MINIO_BUCKET,\n",
    "        \"minio_output_prefix\": MINIO_OUTPUT_PREFIX\n",
    "    }\n",
    "\n",
    "# Update parameters for this step\n",
    "params.update({\n",
    "    \"preprocessing_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"preprocessing_options\": {\n",
    "        \"mean_subtraction\": True,\n",
    "        \"normalization\": True,\n",
    "        \"memory_efficient\": MEMORY_EFFICIENT,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"downsample\": DOWNSAMPLE,\n",
    "        \"downsample_factor\": DOWNSAMPLE_FACTOR if DOWNSAMPLE else None\n",
    "    }\n",
    "})\n",
    "\n",
    "# Verify the previous step completed\n",
    "try:\n",
    "    marker_key = f\"{MINIO_OUTPUT_PREFIX}/data_download_completed.txt\"\n",
    "    s3.head_object(Bucket=MINIO_BUCKET, Key=marker_key)\n",
    "    print(\"Previous step (data fetching) completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Previous step completion marker not found: {str(e)}\")\n",
    "    print(\"Continuing anyway...\")\n",
    "\n",
    "# List objects in the data directory to check what's available\n",
    "print(\"\\nListing files in MinIO data directory:\")\n",
    "data_prefix = f\"{MINIO_OUTPUT_PREFIX}/data/\"\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=data_prefix)\n",
    "\n",
    "# Print found objects\n",
    "if 'Contents' in response:\n",
    "    print(f\"Found {len(response['Contents'])} objects in {MINIO_BUCKET}/{data_prefix}:\")\n",
    "    for obj in response['Contents']:\n",
    "        print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "else:\n",
    "    print(f\"No objects found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "    # Continue with sample data creation\n",
    "\n",
    "if 'Contents' not in response or len(response.get('Contents', [])) == 0:\n",
    "    print(f\"Warning: No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "    print(\"Creating sample data since none was found...\")\n",
    "    \n",
    "    # Create a sample dataset for testing (same code as in fetch_data notebook)\n",
    "    n_snapshots = 50\n",
    "    n_x, n_y = 100, 50\n",
    "    \n",
    "    # Create the velocity fields\n",
    "    velocity = np.zeros((n_snapshots, n_x, n_y, 2))\n",
    "    \n",
    "    # Add a simple flow pattern\n",
    "    x = np.linspace(0, 10, n_x)\n",
    "    y = np.linspace(-2.5, 2.5, n_y)\n",
    "    X, Y = np.meshgrid(x, y, indexing='ij')\n",
    "    \n",
    "    # Generate time-varying flow\n",
    "    for t in range(n_snapshots):\n",
    "        # Base flow from left to right\n",
    "        u = np.ones_like(X) * 1.0\n",
    "        v = np.zeros_like(Y)\n",
    "        \n",
    "        # Add cylinder at x=2\n",
    "        cylinder_x, cylinder_y = 2, 0\n",
    "        cylinder_radius = 0.5\n",
    "        distance = np.sqrt((X - cylinder_x)**2 + (Y - cylinder_y)**2)\n",
    "        mask = distance < cylinder_radius\n",
    "        u[mask] = 0\n",
    "        v[mask] = 0\n",
    "        \n",
    "        # Add oscillating wake\n",
    "        wake_mask = (X > cylinder_x) & (distance > cylinder_radius)\n",
    "        phase = 0.2 * t\n",
    "        v[wake_mask] = 0.3 * np.sin(0.5 * (X[wake_mask] - cylinder_x) + phase) * np.exp(-(Y[wake_mask]**2) / 0.5)\n",
    "        \n",
    "        # Add some random noise\n",
    "        u += 0.02 * np.random.randn(*u.shape)\n",
    "        v += 0.02 * np.random.randn(*v.shape)\n",
    "        \n",
    "        # Store u and v components\n",
    "        velocity[t, :, :, 0] = u\n",
    "        velocity[t, :, :, 1] = v\n",
    "    \n",
    "    # Save to HDF5 file\n",
    "    sample_file = os.path.join(temp_dir, 'cylinder_flow.h5')\n",
    "    with h5py.File(sample_file, 'w') as f:\n",
    "        f.create_dataset('velocity', data=velocity)\n",
    "        f.create_dataset('x', data=x)\n",
    "        f.create_dataset('y', data=y)\n",
    "        f.create_dataset('time', data=np.linspace(0, 10, n_snapshots))\n",
    "    \n",
    "    print(f\"Created sample dataset at {sample_file}\")\n",
    "    \n",
    "    # Upload to MinIO\n",
    "    s3_key = f\"{MINIO_OUTPUT_PREFIX}/data/cylinder_flow.h5\"\n",
    "    s3.upload_file(sample_file, MINIO_BUCKET, s3_key)\n",
    "    print(f\"Uploaded sample dataset to {s3_key}\")\n",
    "    \n",
    "    # Refresh the response\n",
    "    response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=data_prefix)\n",
    "\n",
    "# Download data files from MinIO\n",
    "print(\"\\nDownloading data files from MinIO:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    if obj['Key'].endswith(('.h5', '.hdf5')):\n",
    "        filename = os.path.basename(obj['Key'])\n",
    "        local_path = os.path.join(temp_dir, filename)\n",
    "        \n",
    "        print(f\"  Downloading {obj['Key']} to {local_path}\")\n",
    "        s3.download_file(MINIO_BUCKET, obj['Key'], local_path)\n",
    "\n",
    "print_memory_usage(\"after downloading data\")\n",
    "\n",
    "# Find HDF5 files\n",
    "h5_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) \n",
    "            if f.endswith('.h5') or f.endswith('.hdf5')]\n",
    "\n",
    "if not h5_files:\n",
    "    raise FileNotFoundError(f\"No HDF5 files found in downloaded data\")\n",
    "\n",
    "print(f\"Found {len(h5_files)} HDF5 files: {[os.path.basename(f) for f in h5_files]}\")\n",
    "\n",
    "# Load data from the first file\n",
    "h5_file = h5_files[0]\n",
    "print(f\"Loading data from {os.path.basename(h5_file)}\")\n",
    "\n",
    "try:\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # Print available datasets\n",
    "        print(\"Available datasets:\")\n",
    "        keys = []\n",
    "        \n",
    "        def print_structure(name, obj):\n",
    "            print(f\"  - {name}: {type(obj).__name__}\", end=\"\")\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                print(f\", Shape: {obj.shape}, Dtype: {obj.dtype}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "            keys.append(name)\n",
    "            \n",
    "        f.visititems(print_structure)\n",
    "            \n",
    "        # Try to find velocity data by identifying the largest dataset\n",
    "        datasets = [(name, f[name]) for name in keys if isinstance(f[name], h5py.Dataset)]\n",
    "        \n",
    "        # Sort datasets by number of dimensions and then by size\n",
    "        datasets.sort(key=lambda x: (-len(x[1].shape), -np.prod(x[1].shape)))\n",
    "        \n",
    "        if not datasets:\n",
    "            raise ValueError(\"No datasets found in the HDF5 file\")\n",
    "        \n",
    "        print(\"\\nPotential data candidates (sorted by size):\")\n",
    "        for name, dataset in datasets:\n",
    "            print(f\"  - {name}: Shape {dataset.shape}, Size {np.prod(dataset.shape)}\")\n",
    "        \n",
    "        # Use the largest dataset as our velocity data\n",
    "        velocity_key = datasets[0][0]\n",
    "        print(f\"\\nUsing '{velocity_key}' as velocity data\")\n",
    "        \n",
    "        # Check data shape before loading to estimate memory usage\n",
    "        data_shape = f[velocity_key].shape\n",
    "        data_size_bytes = np.prod(data_shape) * np.dtype(f[velocity_key].dtype).itemsize\n",
    "        data_size_mb = data_size_bytes / (1024 * 1024)\n",
    "        print(f\"Data shape: {data_shape}, Estimated memory required: {data_size_mb:.1f} MB\")\n",
    "        \n",
    "        # If dataset is large, don't load it all at once\n",
    "        if MEMORY_EFFICIENT and data_size_mb > 1000:  # More than 1GB\n",
    "            print(\"Dataset is large - will process in chunks later\")\n",
    "            data = None\n",
    "            # Just store the shape and dtype for later chunked processing\n",
    "            data_dtype = f[velocity_key].dtype\n",
    "            \n",
    "            # Try to identify coordinate data if available\n",
    "            coord_data = {}\n",
    "            coord_keys = [\"x\", \"y\", \"z\", \"X\", \"Y\", \"Z\", \"coord\", \"coords\", \"coordinates\", \"grid\"]\n",
    "            for key in coord_keys:\n",
    "                if key in f:\n",
    "                    coord_data[key] = f[key][:]\n",
    "                    print(f\"Found coordinates '{key}': {coord_data[key].shape}\")\n",
    "            \n",
    "            # If we have time data, save it\n",
    "            if \"time\" in f:\n",
    "                time_data = f[\"time\"][:]\n",
    "                print(f\"Found time data: {time_data.shape}\")\n",
    "            elif \"t\" in f:\n",
    "                time_data = f[\"t\"][:]\n",
    "                print(f\"Found time data: {time_data.shape}\")\n",
    "            else:\n",
    "                time_data = None\n",
    "        else:\n",
    "            # Load full dataset if it's small enough\n",
    "            data = f[velocity_key][:]\n",
    "            \n",
    "            # Optionally downsample the data to reduce memory usage\n",
    "            if DOWNSAMPLE and len(data_shape) >= 3:\n",
    "                if len(data_shape) == 4:  # (time, x, y, components)\n",
    "                    print(f\"Downsampling data by factor of {DOWNSAMPLE_FACTOR}\")\n",
    "                    data = data[:, ::DOWNSAMPLE_FACTOR, ::DOWNSAMPLE_FACTOR, :]\n",
    "                    print(f\"Shape reduced from {data_shape} to {data.shape}\")\n",
    "                \n",
    "                elif len(data_shape) == 3:  # (time, x, y) or similar\n",
    "                    print(f\"Downsampling data by factor of {DOWNSAMPLE_FACTOR}\")\n",
    "                    data = data[:, ::DOWNSAMPLE_FACTOR, ::DOWNSAMPLE_FACTOR]\n",
    "                    print(f\"Shape reduced from {data_shape} to {data.shape}\")\n",
    "            \n",
    "            print(f\"Data successfully loaded\")\n",
    "            \n",
    "            # Try to identify coordinate data if available\n",
    "            coord_data = {}\n",
    "            coord_keys = [\"x\", \"y\", \"z\", \"X\", \"Y\", \"Z\", \"coord\", \"coords\", \"coordinates\", \"grid\"]\n",
    "            for key in coord_keys:\n",
    "                if key in f:\n",
    "                    coord_data[key] = f[key][:]\n",
    "                    print(f\"Found coordinates '{key}': {coord_data[key].shape}\")\n",
    "            \n",
    "            # If we have time data, save it\n",
    "            if \"time\" in f:\n",
    "                time_data = f[\"time\"][:]\n",
    "                print(f\"Found time data: {time_data.shape}\")\n",
    "            elif \"t\" in f:\n",
    "                time_data = f[\"t\"][:]\n",
    "                print(f\"Found time data: {time_data.shape}\")\n",
    "            else:\n",
    "                time_data = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print_memory_usage(\"after loading data\")\n",
    "\n",
    "# Determine data dimensions and structure based on the shape\n",
    "if data is None:\n",
    "    # Working with chunks\n",
    "    shape = data_shape\n",
    "else:\n",
    "    shape = data.shape\n",
    "\n",
    "# Different cases based on data dimensionality\n",
    "if len(shape) == 2:  # (time, points) or (points, time)\n",
    "    if shape[0] < shape[1]:  # Likely (time, points)\n",
    "        n_snapshots = shape[0]\n",
    "        n_points = shape[1]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected (time, points) format: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (points, time)\n",
    "        n_snapshots = shape[1]\n",
    "        n_points = shape[0]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected (points, time) format: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "\n",
    "elif len(shape) == 3:\n",
    "    if shape[0] < shape[1] and shape[0] < shape[2]:  # Likely (time, x, y)\n",
    "        n_snapshots = shape[0]\n",
    "        grid_shape = (shape[1], shape[2])\n",
    "        n_points = np.prod(grid_shape)\n",
    "        n_dims = 1\n",
    "        structured_grid = True\n",
    "        print(f\"Detected (time, x, y) format: {n_snapshots} snapshots, grid shape {grid_shape}, {n_dims} dimensions\")\n",
    "    \n",
    "    elif shape[2] < shape[0] and shape[2] < shape[1]:  # Likely (x, y, time)\n",
    "        n_snapshots = shape[2]\n",
    "        grid_shape = (shape[0], shape[1])\n",
    "        n_points = np.prod(grid_shape)\n",
    "        n_dims = 1\n",
    "        structured_grid = True\n",
    "        print(f\"Detected (x, y, time) format: {n_snapshots} snapshots, grid shape {grid_shape}, {n_dims} dimensions\")\n",
    "    \n",
    "    elif shape[1] == 2 or shape[1] == 3:  # Likely (points, dimensions, time) \n",
    "        n_snapshots = shape[2]\n",
    "        n_points = shape[0]\n",
    "        n_dims = shape[1]\n",
    "        structured_grid = False\n",
    "        print(f\"Detected (points, dimensions, time) format: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    \n",
    "    elif shape[2] == 2 or shape[2] == 3:  # Likely (time, points, dimensions)\n",
    "        n_snapshots = shape[0]\n",
    "        n_points = shape[1]\n",
    "        n_dims = shape[2]\n",
    "        structured_grid = False\n",
    "        print(f\"Detected (time, points, dimensions) format: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Could not determine data structure from shape {shape}\")\n",
    "\n",
    "elif len(shape) == 4:  # Likely (time, x, y, components) \n",
    "    if shape[0] < shape[1] and shape[0] < shape[2]:  # Typical for time series\n",
    "        n_snapshots = shape[0]\n",
    "        grid_shape = (shape[1], shape[2])\n",
    "        n_points = np.prod(grid_shape)\n",
    "        n_dims = shape[3]\n",
    "        structured_grid = True\n",
    "        print(f\"Detected (time, x, y, components) format: {n_snapshots} snapshots, grid shape {grid_shape}, {n_dims} dimensions\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not determine data structure from shape {shape}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported data shape: {shape}\")\n",
    "\n",
    "# Create snapshot matrix and process mean/fluctuations\n",
    "if MEMORY_EFFICIENT and data is None:\n",
    "    print(f\"Processing data in chunks of {CHUNK_SIZE} snapshots to save memory\")\n",
    "    # Initialize arrays for the final results\n",
    "    snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "    mean_flow = np.zeros((n_points * n_dims, 1))\n",
    "    \n",
    "    # Process data in chunks\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # We'll compute the mean in a streaming fashion\n",
    "        chunk_means = []\n",
    "        chunk_weights = []\n",
    "\n",
    "        for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "            chunk_size = chunk_end - chunk_start\n",
    "            print(f\"Processing chunk {chunk_start//CHUNK_SIZE + 1} of {(n_snapshots-1)//CHUNK_SIZE + 1}: snapshots {chunk_start+1}-{chunk_end}\")\n",
    "            \n",
    "            # Load this chunk of data\n",
    "            if len(shape) == 4:  # (time, x, y, components)\n",
    "                chunk_data = f[velocity_key][chunk_start:chunk_end]\n",
    "                # Reshape to (points*dims, chunk_size)\n",
    "                chunk_reshaped = np.zeros((n_points * n_dims, chunk_size))\n",
    "                for i in range(chunk_size):\n",
    "                    chunk_reshaped[:, i] = chunk_data[i].reshape(n_points, n_dims).flatten()\n",
    "            else:\n",
    "                # Handle other dimensionalities similarly\n",
    "                # This is a placeholder - add specific handling for other formats\n",
    "                raise NotImplementedError(f\"Chunked processing not implemented for shape {shape}\")\n",
    "            \n",
    "            # Copy to snapshot matrix\n",
    "            snapshot_matrix[:, chunk_start:chunk_end] = chunk_reshaped\n",
    "            \n",
    "            # Compute chunk mean for streaming mean calculation\n",
    "            chunk_mean = np.mean(chunk_reshaped, axis=1, keepdims=True)\n",
    "            chunk_means.append(chunk_mean)\n",
    "            chunk_weights.append(chunk_size)\n",
    "            \n",
    "            # Free memory\n",
    "            del chunk_data\n",
    "            del chunk_reshaped\n",
    "            gc.collect()\n",
    "            print_memory_usage(f\"after processing chunk {chunk_start//CHUNK_SIZE + 1}\")\n",
    "        \n",
    "        # Compute global mean from chunk means\n",
    "        total_weight = sum(chunk_weights)\n",
    "        for mean, weight in zip(chunk_means, chunk_weights):\n",
    "            mean_flow += (mean * weight / total_weight)\n",
    "        \n",
    "        # Free memory\n",
    "        del chunk_means\n",
    "        del chunk_weights\n",
    "        gc.collect()\n",
    "    \n",
    "    # Compute fluctuations\n",
    "    print(\"Computing fluctuations...\")\n",
    "    if params[\"preprocessing_options\"][\"mean_subtraction\"]:\n",
    "        # Process fluctuations in chunks to save memory\n",
    "        fluctuations = np.zeros_like(snapshot_matrix)\n",
    "        for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "            print(f\"Computing fluctuations for chunk {chunk_start//CHUNK_SIZE + 1}\")\n",
    "            fluctuations[:, chunk_start:chunk_end] = snapshot_matrix[:, chunk_start:chunk_end] - mean_flow\n",
    "        print(\"Mean subtraction applied\")\n",
    "    else:\n",
    "        fluctuations = snapshot_matrix\n",
    "        print(\"Mean subtraction skipped\")\n",
    "    \n",
    "    # Normalize the data if needed\n",
    "    if params[\"preprocessing_options\"][\"normalization\"]:\n",
    "        # Compute norm in chunks\n",
    "        print(\"Computing Frobenius norm...\")\n",
    "        frob_norm_sq = 0\n",
    "        for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "            chunk = fluctuations[:, chunk_start:chunk_end]\n",
    "            frob_norm_sq += np.sum(chunk**2)\n",
    "        \n",
    "        frob_norm = np.sqrt(frob_norm_sq)\n",
    "        print(f\"Normalizing data (Frobenius norm: {frob_norm:.4f})...\")\n",
    "        \n",
    "        # Normalize in chunks\n",
    "        for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "            fluctuations[:, chunk_start:chunk_end] /= frob_norm\n",
    "        \n",
    "        print(f\"Normalization applied\")\n",
    "        \n",
    "        # Save the normalization factor for later use\n",
    "        params[\"preprocessing_options\"][\"normalization_factor\"] = float(frob_norm)\n",
    "    else:\n",
    "        print(\"Normalization skipped\")\n",
    "    \n",
    "else:\n",
    "    # Original code for small datasets or if MEMORY_EFFICIENT is False\n",
    "    # Create snapshot matrix - reshape data into a 2D matrix\n",
    "    # For POD, we need a matrix of shape (n_points*n_dims, n_snapshots)\n",
    "    if len(shape) == 2:\n",
    "        if shape[0] < shape[1]:  # (time, points)\n",
    "            snapshot_matrix = data.T\n",
    "        else:  # (points, time)\n",
    "            snapshot_matrix = data\n",
    "    elif len(shape) == 3:\n",
    "        if shape[0] < shape[1] and shape[0] < shape[2]:  # (time, x, y)\n",
    "            snapshot_matrix = np.zeros((n_points, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                snapshot_matrix[:, i] = data[i].flatten()\n",
    "        elif shape[2] < shape[0] and shape[2] < shape[1]:  # (x, y, time)\n",
    "            snapshot_matrix = np.zeros((n_points, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                snapshot_matrix[:, i] = data[:, :, i].flatten()\n",
    "        elif shape[1] == 2 or shape[1] == 3:  # (points, dimensions, time)\n",
    "            snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                snapshot_matrix[:, i] = data[:, :, i].flatten()\n",
    "        elif shape[2] == 2 or shape[2] == 3:  # (time, points, dimensions)\n",
    "            snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                snapshot_matrix[:, i] = data[i].flatten()\n",
    "    elif len(shape) == 4:  # (time, x, y, components)\n",
    "        snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "        for i in range(n_snapshots):\n",
    "            snapshot_matrix[:, i] = data[i].reshape(n_points, n_dims).flatten()\n",
    "    \n",
    "    print(f\"Created snapshot matrix with shape: {snapshot_matrix.shape}\")\n",
    "    print_memory_usage(\"after creating snapshot matrix\")\n",
    "    \n",
    "    # Compute mean flow\n",
    "    mean_flow = np.mean(snapshot_matrix, axis=1, keepdims=True)\n",
    "    print(f\"Mean flow shape: {mean_flow.shape}\")\n",
    "    \n",
    "    # Subtract mean (centering the data)\n",
    "    if params[\"preprocessing_options\"][\"mean_subtraction\"]:\n",
    "        fluctuations = snapshot_matrix - mean_flow\n",
    "        print(\"Mean subtraction applied\")\n",
    "    else:\n",
    "        fluctuations = snapshot_matrix\n",
    "        print(\"Mean subtraction skipped\")\n",
    "    \n",
    "    # Normalize the data if needed\n",
    "    if params[\"preprocessing_options\"][\"normalization\"]:\n",
    "        # Compute the Frobenius norm of the fluctuation matrix\n",
    "        frob_norm = np.linalg.norm(fluctuations)\n",
    "        # Normalize\n",
    "        fluctuations = fluctuations / frob_norm\n",
    "        print(f\"Normalization applied (Frobenius norm: {frob_norm:.4f})\")\n",
    "        \n",
    "        # Save the normalization factor for later use\n",
    "        params[\"preprocessing_options\"][\"normalization_factor\"] = float(frob_norm)\n",
    "    else:\n",
    "        print(\"Normalization skipped\")\n",
    "\n",
    "print_memory_usage(\"after processing\")\n",
    "\n",
    "# Free memory that's not needed anymore\n",
    "if data is not None:\n",
    "    del data\n",
    "gc.collect()\n",
    "print_memory_usage(\"after cleanup\")\n",
    "\n",
    "# Visualize the mean flow\n",
    "if structured_grid:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if n_dims == 1:\n",
    "        # Scalar field\n",
    "        plt.imshow(mean_flow.reshape(grid_shape), cmap='viridis')\n",
    "        plt.colorbar(label='Mean Velocity')\n",
    "    else:\n",
    "        # Vector field - plot magnitude\n",
    "        mean_reshaped = mean_flow.reshape(grid_shape[0], grid_shape[1], n_dims)\n",
    "        mean_magnitude = np.sqrt(np.sum(mean_reshaped**2, axis=2))\n",
    "        plt.imshow(mean_magnitude, cmap='viridis')\n",
    "        plt.colorbar(label='Mean Velocity Magnitude')\n",
    "        \n",
    "    plt.title('Mean Flow')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization to local file\n",
    "    mean_flow_img = os.path.join(temp_dir, 'mean_flow.png')\n",
    "    plt.savefig(mean_flow_img)\n",
    "    \n",
    "    # Upload to MinIO\n",
    "    s3.upload_file(\n",
    "        mean_flow_img, \n",
    "        MINIO_BUCKET, \n",
    "        f\"{MINIO_OUTPUT_PREFIX}/visualizations/mean_flow.png\"\n",
    "    )\n",
    "    print(f\"Mean flow visualization uploaded to {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/visualizations/mean_flow.png\")\n",
    "\n",
    "# Visualize the first fluctuation mode\n",
    "if structured_grid:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if n_dims == 1:\n",
    "        # Scalar field\n",
    "        plt.imshow(fluctuations[:, 0].reshape(grid_shape), cmap='RdBu_r')\n",
    "        plt.colorbar(label='Velocity Fluctuation')\n",
    "    else:\n",
    "        # Vector field - plot magnitude\n",
    "        fluct_reshaped = fluctuations[:, 0].reshape(grid_shape[0], grid_shape[1], n_dims)\n",
    "        fluct_magnitude = np.sqrt(np.sum(fluct_reshaped**2, axis=2))\n",
    "        plt.imshow(fluct_magnitude, cmap='viridis')\n",
    "        plt.colorbar(label='Velocity Fluctuation Magnitude')\n",
    "        \n",
    "    plt.title('First Fluctuation Snapshot')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization to local file\n",
    "    fluct_img = os.path.join(temp_dir, 'first_fluctuation.png')\n",
    "    plt.savefig(fluct_img)\n",
    "    \n",
    "    # Upload to MinIO\n",
    "    s3.upload_file(\n",
    "        fluct_img, \n",
    "        MINIO_BUCKET, \n",
    "        f\"{MINIO_OUTPUT_PREFIX}/visualizations/first_fluctuation.png\"\n",
    "    )\n",
    "    print(f\"First fluctuation visualization uploaded to {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/visualizations/first_fluctuation.png\")\n",
    "\n",
    "# Make sure preprocessed directory exists in MinIO\n",
    "try:\n",
    "    s3.put_object(Bucket=MINIO_BUCKET, Key=f\"{MINIO_OUTPUT_PREFIX}/preprocessed/\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create preprocessed directory: {str(e)}\")\n",
    "\n",
    "# Save preprocessed data to local files then upload to MinIO\n",
    "print(\"Saving processed data...\")\n",
    "\n",
    "# Save snapshot matrix\n",
    "snapshot_path = os.path.join(temp_dir, 'snapshot_matrix.npy')\n",
    "np.save(snapshot_path, snapshot_matrix)\n",
    "s3.upload_file(\n",
    "    snapshot_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessed/snapshot_matrix.npy\"\n",
    ")\n",
    "\n",
    "# Save mean flow\n",
    "mean_flow_path = os.path.join(temp_dir, 'mean_flow.npy')\n",
    "np.save(mean_flow_path, mean_flow)\n",
    "s3.upload_file(\n",
    "    mean_flow_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessed/mean_flow.npy\"\n",
    ")\n",
    "\n",
    "# Save fluctuations\n",
    "fluctuations_path = os.path.join(temp_dir, 'fluctuations.npy')\n",
    "np.save(fluctuations_path, fluctuations)\n",
    "s3.upload_file(\n",
    "    fluctuations_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessed/fluctuations.npy\"\n",
    ")\n",
    "\n",
    "# Save metadata (convert NumPy values to Python natives for JSON serialization)\n",
    "metadata = {\n",
    "    \"n_snapshots\": int(n_snapshots),\n",
    "    \"n_points\": int(n_points),\n",
    "    \"n_dims\": int(n_dims),\n",
    "    \"structured_grid\": bool(structured_grid),\n",
    "    \"preprocessing_params\": params[\"preprocessing_options\"]\n",
    "}\n",
    "\n",
    "if structured_grid:\n",
    "    metadata[\"grid_shape\"] = tuple(map(int, grid_shape))\n",
    "\n",
    "metadata_path = os.path.join(temp_dir, 'metadata.json')\n",
    "\n",
    "# Use our custom function to save with proper type conversion\n",
    "save_dict_to_json(metadata, metadata_path)\n",
    "\n",
    "s3.upload_file(\n",
    "    metadata_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessed/metadata.json\"\n",
    ")\n",
    "\n",
    "# Update and save parameters for the next step\n",
    "params[\"preprocessing_metadata\"] = metadata\n",
    "params_path = os.path.join(temp_dir, 'params.json')\n",
    "\n",
    "# Use our custom function to save with proper type conversion\n",
    "save_dict_to_json(params, params_path)\n",
    "\n",
    "s3.upload_file(\n",
    "    params_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    ")\n",
    "\n",
    "# Upload completion marker\n",
    "with open(os.path.join(temp_dir, 'preprocessing_completed.txt'), 'w') as f:\n",
    "    f.write(\"Preprocessing completed successfully\")\n",
    "\n",
    "s3.upload_file(\n",
    "    os.path.join(temp_dir, 'preprocessing_completed.txt'),\n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessing_completed.txt\"\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing completed successfully!\")\n",
    "print(f\"Preprocessed data uploaded to: {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/preprocessed/\")\n",
    "\n",
    "# List objects in the bucket to verify uploads\n",
    "print(\"\\nVerifying uploaded files:\")\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=f\"{MINIO_OUTPUT_PREFIX}/preprocessed/\")\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "else:\n",
    "    print(\"No objects found in preprocessed location\")\n",
    "\n",
    "print_memory_usage(\"at end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
