{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0046bf-ad60-42d6-b99a-2985374581bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing boto3...\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.9/site-packages (1.38.21)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from boto3) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.21 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.38.21)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (1.26.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.21->boto3) (1.16.0)\n",
      "Successfully installed boto3\n",
      "Installing h5py...\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.9/site-packages (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.9/site-packages (from h5py) (1.24.2)\n",
      "Successfully installed h5py\n",
      "Installing matplotlib...\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.7.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (5.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Successfully installed matplotlib\n",
      "Installing psutil...\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (5.9.4)\n",
      "Successfully installed psutil\n",
      "Installing numpy...\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.24.2)\n",
      "Successfully installed numpy\n",
      "Dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package, version=None):\n",
    "    \"\"\"Install a package with specific version if needed.\"\"\"\n",
    "    if version:\n",
    "        package_with_version = f\"{package}=={version}\"\n",
    "    else:\n",
    "        package_with_version = package\n",
    "    \n",
    "    print(f\"Installing {package_with_version}...\")\n",
    "    # Allow all packages to install their dependencies\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_with_version])\n",
    "    print(f\"Successfully installed {package_with_version}\")\n",
    "\n",
    "# Only install non-standard library packages\n",
    "install_package(\"boto3\")       # For S3/MinIO operations (will install botocore)\n",
    "install_package(\"h5py\")        # For HDF5 file operations\n",
    "install_package(\"matplotlib\")  # For plotting\n",
    "install_package(\"psutil\")      # For memory tracking\n",
    "install_package(\"numpy\")       # For numerical operations\n",
    "\n",
    "print(\"Dependencies installed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a772d-c79e-49a5-9e77-0fdea33c2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2_rom_modeling.ipynb\n",
    "#\n",
    "# This notebook preprocesses the Cylinder Flow Dataset with memory optimizations:\n",
    "# - Loads the data from MinIO\n",
    "# - Extracts velocity fields\n",
    "# - Creates the snapshot matrix\n",
    "# - Performs mean subtraction\n",
    "# - Normalizes the data if needed\n",
    "# - Uploads processed data back to MinIO\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import tempfile\n",
    "import io\n",
    "import gc  # For garbage collection\n",
    "import psutil  # For memory tracking\n",
    "\n",
    "# Memory management flags\n",
    "MEMORY_EFFICIENT = True\n",
    "CHUNK_SIZE = 10  # Process this many snapshots at a time\n",
    "ENABLE_MEMORY_TRACKING = True\n",
    "DOWNSAMPLE = False\n",
    "DOWNSAMPLE_FACTOR = 2  # Only use every Nth point in grid\n",
    "\n",
    "# Thread control to limit memory usage\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = \"2\"\n",
    "\n",
    "# Function to track memory usage\n",
    "def print_memory_usage(label=\"\"):\n",
    "    if ENABLE_MEMORY_TRACKING:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"Memory usage {label}: {memory_mb:.1f} MB\")\n",
    "\n",
    "print_memory_usage(\"at start\")\n",
    "\n",
    "# Function to convert NumPy types to Python native types for JSON serialization\n",
    "def json_serialize(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, tuple) and hasattr(obj, '_asdict'):\n",
    "        # Handle named tuples\n",
    "        return obj._asdict()\n",
    "    elif isinstance(obj, tuple):\n",
    "        return list(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def save_dict_to_json(data_dict, filepath):\n",
    "    \"\"\"Save dictionary to JSON with NumPy value conversion\"\"\"\n",
    "    # Convert data to JSON-serializable types\n",
    "    serializable_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        serializable_dict[key] = json_serialize(value)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(serializable_dict, f, indent=2)\n",
    "\n",
    "# Create a temporary local directory for processing\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Connect to MinIO\n",
    "print(\"Connecting to MinIO...\")\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT', 'http://minio.minio-system.svc.cluster.local:9000')\n",
    "\n",
    "# Fix the endpoint URL if the protocol is missing\n",
    "if s3_endpoint and not s3_endpoint.startswith(('http://', 'https://')):\n",
    "    s3_endpoint = f\"http://{s3_endpoint}\"\n",
    "    print(f\"Adding http:// prefix to endpoint: {s3_endpoint}\")\n",
    "\n",
    "s3_access_key = os.environ.get('AWS_ACCESS_KEY_ID', 'minio')\n",
    "s3_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY', 'minio123')\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url=s3_endpoint,\n",
    "                  aws_access_key_id=s3_access_key,\n",
    "                  aws_secret_access_key=s3_secret_key,\n",
    "                  config=Config(signature_version='s3v4'))\n",
    "\n",
    "# Load parameters from previous step\n",
    "MINIO_BUCKET = 'rom-data'\n",
    "MINIO_OUTPUT_PREFIX = 'rom-pipeline/outputs'\n",
    "\n",
    "# Download parameters file from MinIO\n",
    "try:\n",
    "    params_key = f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "    params_path = os.path.join(temp_dir, 'params.json')\n",
    "    s3.download_file(MINIO_BUCKET, params_key, params_path)\n",
    "    \n",
    "    with open(params_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "        \n",
    "    print(f\"Loaded parameters successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading parameters, using defaults: {str(e)}\")\n",
    "    params = {\n",
    "        \"dataset_name\": \"cylinder\",\n",
    "        \"minio_bucket\": MINIO_BUCKET,\n",
    "        \"minio_output_prefix\": MINIO_OUTPUT_PREFIX\n",
    "    }\n",
    "\n",
    "# Update parameters for this step\n",
    "params.update({\n",
    "    \"preprocessing_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"preprocessing_options\": {\n",
    "        \"mean_subtraction\": True,\n",
    "        \"normalization\": True,\n",
    "        \"memory_efficient\": MEMORY_EFFICIENT,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"downsample\": DOWNSAMPLE,\n",
    "        \"downsample_factor\": DOWNSAMPLE_FACTOR if DOWNSAMPLE else None\n",
    "    }\n",
    "})\n",
    "\n",
    "# Verify the previous step completed\n",
    "try:\n",
    "    marker_key = f\"{MINIO_OUTPUT_PREFIX}/data_download_completed.txt\"\n",
    "    s3.head_object(Bucket=MINIO_BUCKET, Key=marker_key)\n",
    "    print(\"Previous step (data fetching) completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Previous step completion marker not found: {str(e)}\")\n",
    "    print(\"Continuing anyway...\")\n",
    "\n",
    "# List objects in the data directory to check what's available\n",
    "print(\"\\nListing files in MinIO data directory:\")\n",
    "data_prefix = f\"{MINIO_OUTPUT_PREFIX}/data/\"\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=data_prefix)\n",
    "\n",
    "if 'Contents' not in response or len(response.get('Contents', [])) == 0:\n",
    "    print(f\"Warning: No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "    print(\"Please ensure that data files have been uploaded.\")\n",
    "    raise FileNotFoundError(f\"No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "\n",
    "# Download data files from MinIO\n",
    "print(\"\\nDownloading data files from MinIO:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    if obj['Key'].endswith(('.h5', '.hdf5')):\n",
    "        filename = os.path.basename(obj['Key'])\n",
    "        local_path = os.path.join(temp_dir, filename)\n",
    "        \n",
    "        print(f\"  Downloading {obj['Key']} to {local_path}\")\n",
    "        s3.download_file(MINIO_BUCKET, obj['Key'], local_path)\n",
    "\n",
    "# Find HDF5 files\n",
    "h5_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) \n",
    "            if f.endswith('.h5') or f.endswith('.hdf5')]\n",
    "\n",
    "if not h5_files:\n",
    "    raise FileNotFoundError(f\"No HDF5 files found in downloaded data\")\n",
    "\n",
    "print(f\"Found {len(h5_files)} HDF5 files: {[os.path.basename(f) for f in h5_files]}\")\n",
    "\n",
    "# Load velocity data from the first file\n",
    "h5_file = h5_files[0]\n",
    "print(f\"Loading data from {os.path.basename(h5_file)}\")\n",
    "\n",
    "# Load the data or just get its shape\n",
    "data = None  # We'll load data in chunks if memory efficient mode is on\n",
    "shape = None\n",
    "velocity_key = None\n",
    "\n",
    "try:\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # First, explore the HDF5 file structure to find the velocity field\n",
    "        print(\"Exploring HDF5 file structure:\")\n",
    "        print(\"Top-level groups/datasets:\")\n",
    "        for key in f.keys():\n",
    "            if isinstance(f[key], h5py.Group):\n",
    "                print(f\"  Group: {key} (contains: {list(f[key].keys())})\")\n",
    "            elif isinstance(f[key], h5py.Dataset):\n",
    "                print(f\"  Dataset: {key} (shape: {f[key].shape}, dtype: {f[key].dtype})\")\n",
    "        \n",
    "        # Define possible velocity field names to check\n",
    "        velocity_keys = ['velocity', 'u', 'v', 'vel', 'flow', 'flowfield', 'vector']\n",
    "        \n",
    "        # Search for velocity data\n",
    "        velocity_key = None\n",
    "        \n",
    "        # Step 1: Check direct keys in root level\n",
    "        for key in velocity_keys:\n",
    "            if key in f:\n",
    "                velocity_key = key\n",
    "                print(f\"Found direct velocity key: {key}\")\n",
    "                break\n",
    "        \n",
    "        # Step 2: Look for keys containing velocity terms\n",
    "        if velocity_key is None:\n",
    "            for key in f.keys():\n",
    "                if any(vk in key.lower() for vk in velocity_keys):\n",
    "                    velocity_key = key\n",
    "                    print(f\"Found key containing velocity term: {key}\")\n",
    "                    break\n",
    "        \n",
    "        # Step 3: Search in groups\n",
    "        if velocity_key is None:\n",
    "            for key in f.keys():\n",
    "                if isinstance(f[key], h5py.Group):\n",
    "                    for subkey in f[key].keys():\n",
    "                        if any(vk == subkey.lower() or vk in subkey.lower() for vk in velocity_keys):\n",
    "                            velocity_key = f\"{key}/{subkey}\"\n",
    "                            print(f\"Found velocity data in group: {velocity_key}\")\n",
    "                            break\n",
    "        \n",
    "        # Step 4: Look for large datasets (likely to be the flow field)\n",
    "        if velocity_key is None:\n",
    "            largest_dataset = None\n",
    "            largest_size = 0\n",
    "            \n",
    "            def find_largest_dataset(name, obj):\n",
    "                global largest_dataset, largest_size\n",
    "                if isinstance(obj, h5py.Dataset):\n",
    "                    # Skip small datasets (likely metadata)\n",
    "                    if obj.size > 1000 and obj.size > largest_size:\n",
    "                        largest_size = obj.size\n",
    "                        largest_dataset = name\n",
    "            \n",
    "            # Traverse the entire file\n",
    "            f.visititems(find_largest_dataset)\n",
    "            \n",
    "            if largest_dataset:\n",
    "                velocity_key = largest_dataset\n",
    "                print(f\"Selected largest dataset as velocity data: {velocity_key} (size: {largest_size})\")\n",
    "        \n",
    "        if velocity_key is None:\n",
    "            raise KeyError(\"Could not find velocity data in the file\")\n",
    "            \n",
    "        print(f\"Loading velocity data from '{velocity_key}'\")\n",
    "        \n",
    "        # Just get the shape if memory efficient\n",
    "        if MEMORY_EFFICIENT:\n",
    "            shape = f[velocity_key].shape\n",
    "            print(f\"Data shape: {shape} (memory efficient mode - not loading full data)\")\n",
    "        else:\n",
    "            data = f[velocity_key][:]\n",
    "            shape = data.shape\n",
    "            print(f\"Data shape: {shape}\")\n",
    "        \n",
    "        # Load any coordinates or metadata if available\n",
    "        x_coords = None\n",
    "        y_coords = None\n",
    "        time_data = None\n",
    "        \n",
    "        # Look for common coordinate names\n",
    "        for coord_name in ['x', 'y', 'z', 'X', 'Y', 'Z', 'coord', 'coords', 'coordinates', 'grid', 'time', 't']:\n",
    "            if coord_name in f:\n",
    "                if coord_name.lower() in ['x', 'X']:\n",
    "                    x_coords = f[coord_name][:]\n",
    "                    print(f\"Found x coordinates: shape {x_coords.shape}\")\n",
    "                elif coord_name.lower() in ['y', 'Y']:\n",
    "                    y_coords = f[coord_name][:]\n",
    "                    print(f\"Found y coordinates: shape {y_coords.shape}\")\n",
    "                elif coord_name.lower() in ['time', 't']:\n",
    "                    time_data = f[coord_name][:]\n",
    "                    print(f\"Found time data: shape {time_data.shape}\")\n",
    "                else:\n",
    "                    print(f\"Found other coordinate data '{coord_name}': shape {f[coord_name].shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print_memory_usage(\"after data exploration\")\n",
    "\n",
    "# Determine data dimensions\n",
    "if len(shape) == 2:  # (time, points) or (points, time)\n",
    "    if shape[0] < shape[1]:  # Likely (time, points)\n",
    "        n_snapshots = shape[0]\n",
    "        n_points = shape[1]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (points, time)\n",
    "        n_snapshots = shape[1]\n",
    "        n_points = shape[0]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "        \n",
    "elif len(shape) == 3:  # (time, x, y) or (time, points, dims)\n",
    "    if shape[1] > shape[2]:  # Likely (time, points, dims)\n",
    "        n_snapshots = shape[0]\n",
    "        n_points = shape[1]\n",
    "        n_dims = shape[2]\n",
    "        structured_grid = False\n",
    "        print(f\"Detected (time, points, dimensions) format: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (time, x, y) for a scalar field\n",
    "        n_snapshots = shape[0]\n",
    "        grid_shape = (shape[1], shape[2])\n",
    "        n_points = grid_shape[0] * grid_shape[1]\n",
    "        n_dims = 1\n",
    "        structured_grid = True\n",
    "        print(f\"Detected structured scalar data: {n_snapshots} snapshots, grid shape {grid_shape}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Could not determine data structure from shape {shape}\")\n",
    "\n",
    "elif len(shape) == 4:  # Likely (time, x, y, components) \n",
    "    if shape[0] < shape[1] and shape[0] < shape[2]:  # Typical for time series\n",
    "        n_snapshots = shape[0]\n",
    "        grid_shape = (shape[1], shape[2])\n",
    "        n_points = np.prod(grid_shape)\n",
    "        n_dims = shape[3]\n",
    "        structured_grid = True\n",
    "        print(f\"Detected (time, x, y, components) format: {n_snapshots} snapshots, grid shape {grid_shape}, {n_dims} dimensions\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not determine data structure from shape {shape}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported data shape: {shape}\")\n",
    "\n",
    "# Create snapshot matrix and process mean/fluctuations\n",
    "if MEMORY_EFFICIENT and data is None:\n",
    "    print(f\"Processing data in chunks of {CHUNK_SIZE} snapshots to save memory\")\n",
    "    # Initialize arrays for the final results\n",
    "    snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "    mean_flow = np.zeros((n_points * n_dims, 1))\n",
    "    \n",
    "    # Process data in chunks\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # We'll compute the mean in a streaming fashion\n",
    "        chunk_means = []\n",
    "        chunk_weights = []\n",
    "\n",
    "        for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "            chunk_size = chunk_end - chunk_start\n",
    "            print(f\"Processing chunk {chunk_start//CHUNK_SIZE + 1} of {(n_snapshots-1)//CHUNK_SIZE + 1}: snapshots {chunk_start+1}-{chunk_end}\")\n",
    "            \n",
    "            # Load this chunk of data\n",
    "            if len(shape) == 4:  # (time, x, y, components)\n",
    "                chunk_data = f[velocity_key][chunk_start:chunk_end]\n",
    "                # Reshape to (points*dims, chunk_size)\n",
    "                chunk_reshaped = np.zeros((n_points * n_dims, chunk_size))\n",
    "                for i in range(chunk_size):\n",
    "                    chunk_reshaped[:, i] = chunk_data[i].reshape(n_points, n_dims).flatten()\n",
    "            elif len(shape) == 3:  # (time, x, y) for scalar field or (time, points, dims)\n",
    "                chunk_data = f[velocity_key][chunk_start:chunk_end]\n",
    "                # Reshape to (points*dims, chunk_size)\n",
    "                chunk_reshaped = np.zeros((n_points * n_dims, chunk_size))\n",
    "                \n",
    "                if structured_grid:  # (time, x, y) for scalar field\n",
    "                    for i in range(chunk_size):\n",
    "                        # Reshape 2D grid to 1D array\n",
    "                        chunk_reshaped[:, i] = chunk_data[i].reshape(n_points)\n",
    "                else:  # (time, points, dims)\n",
    "                    for i in range(chunk_size):\n",
    "                        # Flatten the points and dimensions\n",
    "                        chunk_reshaped[:, i] = chunk_data[i].reshape(n_points * n_dims)\n",
    "            elif len(shape) == 2:  # (time, points) or (points, time)\n",
    "                if shape[0] < shape[1]:  # (time, points)\n",
    "                    chunk_data = f[velocity_key][chunk_start:chunk_end]\n",
    "                    chunk_reshaped = chunk_data.T  # Transpose to (points, chunk_size)\n",
    "                else:  # (points, time)\n",
    "                    chunk_data = f[velocity_key][:, chunk_start:chunk_end]\n",
    "                    chunk_reshaped = chunk_data  # Already (points, chunk_size)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported data shape for chunked processing: {shape}\")\n",
    "            \n",
    "            # Copy to snapshot matrix\n",
    "            snapshot_matrix[:, chunk_start:chunk_end] = chunk_reshaped\n",
    "            \n",
    "            # Compute chunk mean for streaming mean calculation\n",
    "            chunk_mean = np.mean(chunk_reshaped, axis=1, keepdims=True)\n",
    "            chunk_means.append(chunk_mean)\n",
    "            chunk_weights.append(chunk_size / n_snapshots)\n",
    "            \n",
    "            # Free memory\n",
    "            del chunk_data\n",
    "            del chunk_reshaped\n",
    "            gc.collect()\n",
    "            print_memory_usage(f\"after processing chunk {chunk_start//CHUNK_SIZE + 1}\")\n",
    "        \n",
    "        # Compute overall mean from chunk means\n",
    "        for i, (chunk_mean, weight) in enumerate(zip(chunk_means, chunk_weights)):\n",
    "            mean_flow += chunk_mean * weight\n",
    "        \n",
    "        print(f\"Computed mean flow with shape: {mean_flow.shape}\")\n",
    "        \n",
    "        # Compute fluctuations\n",
    "        print(\"Computing fluctuations...\")\n",
    "        fluctuations = np.zeros_like(snapshot_matrix)\n",
    "        \n",
    "        for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "            print(f\"Processing fluctuations for chunk {chunk_start//CHUNK_SIZE + 1}\")\n",
    "            \n",
    "            # Subtract mean\n",
    "            fluctuations[:, chunk_start:chunk_end] = snapshot_matrix[:, chunk_start:chunk_end] - mean_flow\n",
    "            \n",
    "            print_memory_usage(f\"after fluctuations for chunk {chunk_start//CHUNK_SIZE + 1}\")\n",
    "else:\n",
    "    # Process all data at once\n",
    "    print(\"Processing all data at once\")\n",
    "    \n",
    "    # Create snapshot matrix - reshape data into a 2D matrix\n",
    "    # For POD, we need a matrix of shape (n_points*n_dims, n_snapshots)\n",
    "    if structured_grid:\n",
    "        if n_dims == 1:\n",
    "            # For scalar field\n",
    "            snapshot_matrix = np.zeros((n_points, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                snapshot_matrix[:, i] = data[i].reshape(n_points)\n",
    "        else:\n",
    "            # For vector field\n",
    "            snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                # Reshape and stack components\n",
    "                reshaped = data[i].reshape(n_points, n_dims)\n",
    "                snapshot_matrix[:, i] = reshaped.flatten()\n",
    "    else:\n",
    "        # Already in the right format for unstructured grid\n",
    "        if n_dims == 1:\n",
    "            if shape[0] < shape[1]:\n",
    "                snapshot_matrix = data.T  # Transpose to get (n_points, n_snapshots)\n",
    "            else:\n",
    "                snapshot_matrix = data  # Already (n_points, n_snapshots)\n",
    "        else:\n",
    "            snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "            for i in range(n_snapshots):\n",
    "                snapshot_matrix[:, i] = data[i].flatten()\n",
    "\n",
    "    print(f\"Created snapshot matrix with shape: {snapshot_matrix.shape}\")\n",
    "    print_memory_usage(\"after creating snapshot matrix\")\n",
    "\n",
    "    # Compute mean flow\n",
    "    mean_flow = np.mean(snapshot_matrix, axis=1, keepdims=True)\n",
    "    print(f\"Mean flow shape: {mean_flow.shape}\")\n",
    "\n",
    "    # Subtract mean (centering the data)\n",
    "    fluctuations = snapshot_matrix - mean_flow\n",
    "    print(\"Mean subtraction applied\")\n",
    "    print_memory_usage(\"after mean subtraction\")\n",
    "\n",
    "# Normalize the data if needed\n",
    "if params[\"preprocessing_options\"].get(\"normalization\", True):\n",
    "    # Compute the Frobenius norm of the fluctuation matrix\n",
    "    if MEMORY_EFFICIENT:\n",
    "        # Compute norm in chunks\n",
    "        frob_norm_sq = 0\n",
    "        for chunk_start in range(0, n_snapshots, CHUNK_SIZE):\n",
    "            chunk_end = min(chunk_start + CHUNK_SIZE, n_snapshots)\n",
    "            chunk = fluctuations[:, chunk_start:chunk_end]\n",
    "            frob_norm_sq += np.sum(chunk**2)\n",
    "        frob_norm = np.sqrt(frob_norm_sq)\n",
    "    else:\n",
    "        frob_norm = np.linalg.norm(fluctuations)\n",
    "    \n",
    "    # Normalize\n",
    "    fluctuations = fluctuations / frob_norm\n",
    "    print(f\"Normalization applied (Frobenius norm: {frob_norm:.4f})\")\n",
    "    \n",
    "    # Save the normalization factor for later use\n",
    "    params[\"preprocessing_options\"][\"normalization_factor\"] = float(frob_norm)\n",
    "else:\n",
    "    print(\"Normalization skipped\")\n",
    "\n",
    "print_memory_usage(\"after normalization\")\n",
    "\n",
    "# Compute SVD for POD modes\n",
    "print(\"\\nComputing SVD for POD modes...\")\n",
    "try:\n",
    "    # Use randomized SVD for large datasets\n",
    "    if n_points * n_dims > 10000 or n_snapshots > 1000:\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        n_modes = min(n_snapshots - 1, 100)  # Limit to 100 modes for very large datasets\n",
    "        print(f\"Using randomized SVD with {n_modes} modes due to large data size\")\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=n_modes, random_state=42)\n",
    "        svd.fit(fluctuations.T)  # TruncatedSVD expects (n_samples, n_features)\n",
    "        \n",
    "        # Extract components\n",
    "        spatial_modes = svd.components_.T  # Shape: (n_points*n_dims, n_modes)\n",
    "        singular_values = svd.singular_values_\n",
    "        temporal_modes = fluctuations.T @ spatial_modes / singular_values  # Shape: (n_snapshots, n_modes)\n",
    "    else:\n",
    "        # Use full SVD for smaller datasets\n",
    "        U, S, Vt = np.linalg.svd(fluctuations, full_matrices=False)\n",
    "        \n",
    "        # Extract components\n",
    "        spatial_modes = U  # Shape: (n_points*n_dims, n_modes)\n",
    "        singular_values = S\n",
    "        temporal_modes = Vt.T  # Shape: (n_snapshots, n_modes)\n",
    "        \n",
    "    print(f\"SVD completed successfully\")\n",
    "    print(f\"Spatial modes shape: {spatial_modes.shape}\")\n",
    "    print(f\"Singular values shape: {singular_values.shape}\")\n",
    "    print(f\"Temporal modes shape: {temporal_modes.shape}\")\n",
    "    \n",
    "    # Compute energy content\n",
    "    energy = singular_values**2\n",
    "    total_energy = np.sum(energy)\n",
    "    energy_ratio = energy / total_energy\n",
    "    cumulative_energy = np.cumsum(energy_ratio)\n",
    "    \n",
    "    print(f\"Total energy: {total_energy:.4f}\")\n",
    "    print(f\"Energy in first mode: {energy[0]:.4f} ({energy_ratio[0]*100:.2f}%)\")\n",
    "    \n",
    "    # Find number of modes for different energy thresholds\n",
    "    for threshold in [0.9, 0.95, 0.99, 0.999]:\n",
    "        n_modes = np.argmax(cumulative_energy >= threshold) + 1\n",
    "        print(f\"Number of modes for {threshold*100:.1f}% energy: {n_modes}\")\n",
    "    \n",
    "    # Save results\n",
    "    pod_results = {\n",
    "        'singular_values': singular_values,\n",
    "        'energy_ratio': energy_ratio,\n",
    "        'cumulative_energy': cumulative_energy,\n",
    "        'n_points': n_points,\n",
    "        'n_dims': n_dims,\n",
    "        'n_snapshots': n_snapshots,\n",
    "        'structured_grid': structured_grid\n",
    "    }\n",
    "    \n",
    "    if structured_grid:\n",
    "        pod_results['grid_shape'] = grid_shape\n",
    "    \n",
    "    # Save modes data\n",
    "    modes_data = {\n",
    "        'spatial_modes': spatial_modes,\n",
    "        'temporal_modes': temporal_modes,\n",
    "        'singular_values': singular_values,\n",
    "        'mean_flow': mean_flow\n",
    "    }\n",
    "    \n",
    "    # Save to NPZ file\n",
    "    pod_results_file = os.path.join(temp_dir, 'pod_results.npz')\n",
    "    print(f\"Saving POD results to {pod_results_file}\")\n",
    "    np.savez_compressed(pod_results_file, **pod_results)\n",
    "    \n",
    "    modes_data_file = os.path.join(temp_dir, 'pod_modes.npz')\n",
    "    print(f\"Saving POD modes to {modes_data_file}\")\n",
    "    np.savez_compressed(modes_data_file, **modes_data)\n",
    "    \n",
    "    # Upload results to MinIO\n",
    "    pod_results_key = f\"{MINIO_OUTPUT_PREFIX}/pod_results.npz\"\n",
    "    print(f\"Uploading POD results to {MINIO_BUCKET}/{pod_results_key}\")\n",
    "    s3.upload_file(pod_results_file, MINIO_BUCKET, pod_results_key)\n",
    "    \n",
    "    modes_data_key = f\"{MINIO_OUTPUT_PREFIX}/pod_modes.npz\"\n",
    "    print(f\"Uploading POD modes to {MINIO_BUCKET}/{modes_data_key}\")\n",
    "    s3.upload_file(modes_data_file, MINIO_BUCKET, modes_data_key)\n",
    "    \n",
    "    # Visualize energy content\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.semilogy(range(1, len(singular_values)+1), singular_values, 'o-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Mode number')\n",
    "    plt.ylabel('Singular value')\n",
    "    plt.title('Singular Values Decay')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(cumulative_energy)+1), cumulative_energy, 'o-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of modes')\n",
    "    plt.ylabel('Cumulative energy ratio')\n",
    "    plt.title('Cumulative Energy Content')\n",
    "    \n",
    "    # Add horizontal lines for thresholds\n",
    "    for threshold in [0.9, 0.95, 0.99]:\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', alpha=0.5)\n",
    "        plt.text(len(cumulative_energy)*0.8, threshold+0.01, f'{threshold*100:.0f}%', color='r')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    energy_plot = os.path.join(temp_dir, 'energy_plot.png')\n",
    "    plt.savefig(energy_plot, dpi=150)\n",
    "    \n",
    "    # Upload visualization\n",
    "    energy_plot_key = f\"{MINIO_OUTPUT_PREFIX}/visualizations/energy_plot.png\"\n",
    "    print(f\"Uploading energy plot to {MINIO_BUCKET}/{energy_plot_key}\")\n",
    "    s3.upload_file(energy_plot, MINIO_BUCKET, energy_plot_key)\n",
    "    \n",
    "    # Visualize a few spatial modes\n",
    "    if structured_grid:\n",
    "        n_vis_modes = min(4, spatial_modes.shape[1])\n",
    "        plt.figure(figsize=(15, 3*n_vis_modes))\n",
    "        \n",
    "        for i in range(n_vis_modes):\n",
    "            plt.subplot(n_vis_modes, 1, i+1)\n",
    "            \n",
    "            if n_dims == 1:\n",
    "                # Scalar field\n",
    "                mode_reshaped = spatial_modes[:, i].reshape(grid_shape)\n",
    "                plt.imshow(mode_reshaped, cmap='RdBu_r')\n",
    "                plt.colorbar(label=f'Mode {i+1}')\n",
    "            else:\n",
    "                # Vector field - plot magnitude\n",
    "                mode_reshaped = spatial_modes[:, i].reshape(grid_shape[0], grid_shape[1], n_dims)\n",
    "                mode_magnitude = np.sqrt(np.sum(mode_reshaped**2, axis=2))\n",
    "                plt.imshow(mode_magnitude, cmap='viridis')\n",
    "                plt.colorbar(label=f'Mode {i+1} Magnitude')\n",
    "                \n",
    "            plt.title(f'Spatial Mode {i+1} (Energy: {energy_ratio[i]*100:.2f}%)')\n",
    "            plt.xlabel('X')\n",
    "            plt.ylabel('Y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization\n",
    "        modes_plot = os.path.join(temp_dir, 'spatial_modes.png')\n",
    "        plt.savefig(modes_plot, dpi=150)\n",
    "        \n",
    "        # Upload visualization\n",
    "        modes_plot_key = f\"{MINIO_OUTPUT_PREFIX}/visualizations/spatial_modes.png\"\n",
    "        print(f\"Uploading spatial modes plot to {MINIO_BUCKET}/{modes_plot_key}\")\n",
    "        s3.upload_file(modes_plot, MINIO_BUCKET, modes_plot_key)\n",
    "    \n",
    "    # Visualize temporal modes\n",
    "    n_vis_modes = min(4, temporal_modes.shape[1])\n",
    "    plt.figure(figsize=(15, 2*n_vis_modes))\n",
    "    \n",
    "    for i in range(n_vis_modes):\n",
    "        plt.subplot(n_vis_modes, 1, i+1)\n",
    "        plt.plot(temporal_modes[:, i])\n",
    "        plt.grid(True)\n",
    "        plt.title(f'Temporal Mode {i+1} (Energy: {energy_ratio[i]*100:.2f}%)')\n",
    "        plt.xlabel('Time step')\n",
    "        plt.ylabel('Amplitude')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    temporal_plot = os.path.join(temp_dir, 'temporal_modes.png')\n",
    "    plt.savefig(temporal_plot, dpi=150)\n",
    "    \n",
    "    # Upload visualization\n",
    "    temporal_plot_key = f\"{MINIO_OUTPUT_PREFIX}/visualizations/temporal_modes.png\"\n",
    "    print(f\"Uploading temporal modes plot to {MINIO_BUCKET}/{temporal_plot_key}\")\n",
    "    s3.upload_file(temporal_plot, MINIO_BUCKET, temporal_plot_key)\n",
    "    \n",
    "    # Update parameters with POD results\n",
    "    params.update({\n",
    "        \"pod_results\": {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"n_modes\": len(singular_values),\n",
    "            \"energy_thresholds\": {\n",
    "                \"90%\": int(np.argmax(cumulative_energy >= 0.9) + 1),\n",
    "                \"95%\": int(np.argmax(cumulative_energy >= 0.95) + 1),\n",
    "                \"99%\": int(np.argmax(cumulative_energy >= 0.99) + 1),\n",
    "                \"99.9%\": int(np.argmax(cumulative_energy >= 0.999) + 1)\n",
    "            },\n",
    "            \"first_mode_energy\": float(energy_ratio[0])\n",
    "        }\n",
    "    })\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error computing SVD: {str(e)}\")\n",
    "    # Continue with the rest of the script\n",
    "    params.update({\n",
    "        \"pod_results\": {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    })\n",
    "\n",
    "print_memory_usage(\"after SVD computation\")\n",
    "\n",
    "# Save parameters\n",
    "params_file = os.path.join(temp_dir, 'params.json')\n",
    "save_dict_to_json(params, params_file)\n",
    "\n",
    "# Upload parameters to MinIO\n",
    "params_key = f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "print(f\"Uploading parameters to {MINIO_BUCKET}/{params_key}\")\n",
    "s3.upload_file(params_file, MINIO_BUCKET, params_key)\n",
    "\n",
    "# Create a completion marker\n",
    "completion_marker = os.path.join(temp_dir, 'rom_modeling_completed.txt')\n",
    "with open(completion_marker, 'w') as f:\n",
    "    f.write(f\"ROM modeling completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Processed {n_snapshots} snapshots with {n_points} spatial points and {n_dims} dimensions\\n\")\n",
    "    if 'pod_results' in params and 'error' not in params['pod_results']:\n",
    "        f.write(f\"Computed {len(singular_values)} POD modes\\n\")\n",
    "        f.write(f\"90% energy captured with {params['pod_results']['energy_thresholds']['90%']} modes\\n\")\n",
    "        f.write(f\"99% energy captured with {params['pod_results']['energy_thresholds']['99%']} modes\\n\")\n",
    "\n",
    "# Upload completion marker to MinIO\n",
    "marker_key = f\"{MINIO_OUTPUT_PREFIX}/rom_modeling_completed.txt\"\n",
    "print(f\"Uploading completion marker to {MINIO_BUCKET}/{marker_key}\")\n",
    "s3.upload_file(completion_marker, MINIO_BUCKET, marker_key)\n",
    "\n",
    "print(\"\\nROM modeling completed successfully!\")\n",
    "print(f\"Processed {n_snapshots} snapshots with {n_points} spatial points and {n_dims} dimensions\")\n",
    "if 'pod_results' in params and 'error' not in params['pod_results']:\n",
    "    print(f\"Computed {len(singular_values)} POD modes\")\n",
    "    print(f\"90% energy captured with {params['pod_results']['energy_thresholds']['90%']} modes\")\n",
    "    print(f\"99% energy captured with {params['pod_results']['energy_thresholds']['99%']} modes\")\n",
    "print(f\"Results saved to {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
