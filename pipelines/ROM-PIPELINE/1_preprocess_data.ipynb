{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324ffc8-2fb4-46e6-abd7-7de10e54aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1_preprocess_data.ipynb\n",
    "#\n",
    "# This notebook preprocesses the Cylinder Flow Dataset:\n",
    "# - Loads the data from MinIO\n",
    "# - Extracts velocity fields\n",
    "# - Creates the snapshot matrix\n",
    "# - Performs mean subtraction\n",
    "# - Normalizes the data if needed\n",
    "# - Uploads processed data back to MinIO\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import tempfile\n",
    "import io\n",
    "\n",
    "# Create a temporary local directory for processing\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Connect to MinIO\n",
    "print(\"Connecting to MinIO...\")\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT', 'http://minio:9000')\n",
    "\n",
    "# Fix the endpoint URL if the protocol is missing\n",
    "if s3_endpoint and not s3_endpoint.startswith(('http://', 'https://')):\n",
    "    s3_endpoint = f\"http://{s3_endpoint}\"\n",
    "    print(f\"Adding http:// prefix to endpoint: {s3_endpoint}\")\n",
    "\n",
    "s3_access_key = os.environ.get('AWS_ACCESS_KEY_ID', 'minioadmin')\n",
    "s3_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY', 'minioadmin')\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url=s3_endpoint,\n",
    "                  aws_access_key_id=s3_access_key,\n",
    "                  aws_secret_access_key=s3_secret_key,\n",
    "                  config=Config(signature_version='s3v4'))\n",
    "\n",
    "# Load parameters from previous step\n",
    "MINIO_BUCKET = 'rom-data'\n",
    "MINIO_OUTPUT_PREFIX = 'rom-pipeline/outputs'\n",
    "\n",
    "# Download parameters file from MinIO\n",
    "try:\n",
    "    params_key = f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "    params_path = os.path.join(temp_dir, 'params.json')\n",
    "    s3.download_file(MINIO_BUCKET, params_key, params_path)\n",
    "    \n",
    "    with open(params_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "        \n",
    "    print(f\"Loaded parameters: {params}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading parameters, using defaults: {str(e)}\")\n",
    "    params = {\n",
    "        \"dataset_name\": \"cylinder\",\n",
    "        \"minio_bucket\": MINIO_BUCKET,\n",
    "        \"minio_output_prefix\": MINIO_OUTPUT_PREFIX\n",
    "    }\n",
    "\n",
    "# Update parameters for this step\n",
    "params.update({\n",
    "    \"preprocessing_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"preprocessing_options\": {\n",
    "        \"mean_subtraction\": True,\n",
    "        \"normalization\": True\n",
    "    }\n",
    "})\n",
    "\n",
    "# Verify the previous step completed\n",
    "try:\n",
    "    marker_key = f\"{MINIO_OUTPUT_PREFIX}/data_download_completed.txt\"\n",
    "    s3.head_object(Bucket=MINIO_BUCKET, Key=marker_key)\n",
    "    print(\"Previous step (data fetching) completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Previous step completion marker not found: {str(e)}\")\n",
    "    print(\"Continuing anyway...\")\n",
    "\n",
    "# List objects in the data directory to check what's available\n",
    "print(\"\\nListing files in MinIO data directory:\")\n",
    "data_prefix = f\"{MINIO_OUTPUT_PREFIX}/data/\"\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=data_prefix)\n",
    "\n",
    "if 'Contents' not in response or len(response.get('Contents', [])) == 0:\n",
    "    print(f\"Warning: No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "    print(\"Please ensure that data files have been uploaded.\")\n",
    "    raise FileNotFoundError(f\"No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "\n",
    "# Download data files from MinIO\n",
    "print(\"\\nDownloading data files from MinIO:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    if obj['Key'].endswith(('.h5', '.hdf5')):\n",
    "        filename = os.path.basename(obj['Key'])\n",
    "        local_path = os.path.join(temp_dir, filename)\n",
    "        \n",
    "        print(f\"  Downloading {obj['Key']} to {local_path}\")\n",
    "        s3.download_file(MINIO_BUCKET, obj['Key'], local_path)\n",
    "\n",
    "# Find HDF5 files\n",
    "h5_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) \n",
    "            if f.endswith('.h5') or f.endswith('.hdf5')]\n",
    "\n",
    "if not h5_files:\n",
    "    raise FileNotFoundError(f\"No HDF5 files found in downloaded data\")\n",
    "\n",
    "print(f\"Found {len(h5_files)} HDF5 files: {[os.path.basename(f) for f in h5_files]}\")\n",
    "\n",
    "# Load velocity data from the first file\n",
    "h5_file = h5_files[0]\n",
    "print(f\"Loading data from {os.path.basename(h5_file)}\")\n",
    "\n",
    "try:\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # First, explore the HDF5 file structure to find the velocity field\n",
    "        print(\"Exploring HDF5 file structure:\")\n",
    "        print(\"Top-level groups/datasets:\")\n",
    "        for key in f.keys():\n",
    "            if isinstance(f[key], h5py.Group):\n",
    "                print(f\"  Group: {key} (contains: {list(f[key].keys())})\")\n",
    "            elif isinstance(f[key], h5py.Dataset):\n",
    "                print(f\"  Dataset: {key} (shape: {f[key].shape}, dtype: {f[key].dtype})\")\n",
    "        \n",
    "        # Define possible velocity field names to check\n",
    "        velocity_keys = ['velocity', 'u', 'v', 'vel', 'flow', 'flowfield', 'vector']\n",
    "        \n",
    "        # Search for velocity data\n",
    "        velocity_key = None\n",
    "        \n",
    "        # Step 1: Check direct keys in root level\n",
    "        for key in velocity_keys:\n",
    "            if key in f:\n",
    "                velocity_key = key\n",
    "                print(f\"Found direct velocity key: {key}\")\n",
    "                break\n",
    "        \n",
    "        # Step 2: Look for keys containing velocity terms\n",
    "        if velocity_key is None:\n",
    "            for key in f.keys():\n",
    "                if any(vk in key.lower() for vk in velocity_keys):\n",
    "                    velocity_key = key\n",
    "                    print(f\"Found key containing velocity term: {key}\")\n",
    "                    break\n",
    "        \n",
    "        # Step 3: Search in groups\n",
    "        if velocity_key is None:\n",
    "            for key in f.keys():\n",
    "                if isinstance(f[key], h5py.Group):\n",
    "                    for subkey in f[key].keys():\n",
    "                        if any(vk == subkey.lower() or vk in subkey.lower() for vk in velocity_keys):\n",
    "                            velocity_key = f\"{key}/{subkey}\"\n",
    "                            print(f\"Found velocity data in group: {velocity_key}\")\n",
    "                            break\n",
    "        \n",
    "        # Step 4: Look for large datasets (likely to be the flow field)\n",
    "        if velocity_key is None:\n",
    "            largest_dataset = None\n",
    "            largest_size = 0\n",
    "            \n",
    "            def find_largest_dataset(name, obj):\n",
    "                nonlocal largest_dataset, largest_size\n",
    "                if isinstance(obj, h5py.Dataset):\n",
    "                    # Skip small datasets (likely metadata)\n",
    "                    if obj.size > 1000 and obj.size > largest_size:\n",
    "                        largest_size = obj.size\n",
    "                        largest_dataset = name\n",
    "            \n",
    "            # Traverse the entire file\n",
    "            f.visititems(find_largest_dataset)\n",
    "            \n",
    "            if largest_dataset:\n",
    "                velocity_key = largest_dataset\n",
    "                print(f\"Selected largest dataset as velocity data: {velocity_key} (size: {largest_size})\")\n",
    "        \n",
    "        if velocity_key is None:\n",
    "            raise KeyError(\"Could not find velocity data in the file\")\n",
    "            \n",
    "        print(f\"Loading velocity data from '{velocity_key}'\")\n",
    "        velocity = f[velocity_key][:]\n",
    "        \n",
    "        # Load any coordinates or metadata if available\n",
    "        x_coords = None\n",
    "        y_coords = None\n",
    "        time_data = None\n",
    "        \n",
    "        # Look for common coordinate names\n",
    "        for coord_name in ['x', 'y', 'z', 'X', 'Y', 'Z', 'coord', 'coords', 'coordinates', 'grid', 'time', 't']:\n",
    "            if coord_name in f:\n",
    "                if coord_name.lower() in ['x', 'X']:\n",
    "                    x_coords = f[coord_name][:]\n",
    "                    print(f\"Found x coordinates: shape {x_coords.shape}\")\n",
    "                elif coord_name.lower() in ['y', 'Y']:\n",
    "                    y_coords = f[coord_name][:]\n",
    "                    print(f\"Found y coordinates: shape {y_coords.shape}\")\n",
    "                elif coord_name.lower() in ['time', 't']:\n",
    "                    time_data = f[coord_name][:]\n",
    "                    print(f\"Found time data: shape {time_data.shape}\")\n",
    "                else:\n",
    "                    print(f\"Found other coordinate data '{coord_name}': shape {f[coord_name].shape}\")\n",
    "        \n",
    "        print(f\"Velocity data shape: {velocity.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Determine data dimensions\n",
    "if len(velocity.shape) == 2:  # (time, points) or (points, time)\n",
    "    if velocity.shape[0] < velocity.shape[1]:  # Likely (time, points)\n",
    "        n_snapshots = velocity.shape[0]\n",
    "        n_points = velocity.shape[1]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (points, time)\n",
    "        n_snapshots = velocity.shape[1]\n",
    "        n_points = velocity.shape[0]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "        \n",
    "elif len(velocity.shape) == 3:  # (time, x, y) or (time, points, dims)\n",
    "    if velocity.shape[1] > velocity.shape[2]:  # Likely (time, points, dims)\n",
    "        n_snapshots = velocity.shape[0]\n",
    "        n_points = velocity.shape[1]\n",
    "        n_dims = velocity.shape[2]\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (time, x, y) for a scalar field\n",
    "        n_snapshots = velocity.shape[0]\n",
    "        grid_shape = (velocity.shape[1], velocity.shape[2])\n",
    "        n_points = grid_shape[0] * grid_shape[1]\n",
    "        structured_grid = True\n",
    "        n_dims = 1\n",
    "        print(f\"Detected structured scalar data: {n_snapshots} snapshots, grid shape {grid_shape}\")\n",
    "        \n",
    "elif len(velocity.shape) == 4:  # (time, x, y, components)\n",
    "    n_snapshots = velocity.shape[0]\n",
    "    grid_shape = (velocity.shape[1], velocity.shape[2])\n",
    "    n_points = grid_shape[0] * grid_shape[1]\n",
    "    n_dims = velocity.shape[3]\n",
    "    structured_grid = True\n",
    "    print(f\"Detected structured vector data: {n_snapshots} snapshots, grid shape {grid_shape}, {n_dims} components\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unexpected velocity data shape: {velocity.shape}\")\n",
    "\n",
    "# Create snapshot matrix - reshape data into a 2D matrix\n",
    "# For POD, we need a matrix of shape (n_points*n_dims, n_snapshots)\n",
    "if structured_grid:\n",
    "    if n_dims == 1:\n",
    "        # For scalar field\n",
    "        snapshot_matrix = np.zeros((n_points, n_snapshots))\n",
    "        for i in range(n_snapshots):\n",
    "            snapshot_matrix[:, i] = velocity[i].reshape(n_points)\n",
    "    else:\n",
    "        # For vector field\n",
    "        snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "        for i in range(n_snapshots):\n",
    "            # Reshape and stack components\n",
    "            reshaped = velocity[i].reshape(n_points, n_dims)\n",
    "            snapshot_matrix[:, i] = reshaped.flatten()\n",
    "else:\n",
    "    # Already in the right format for unstructured grid\n",
    "    if n_dims == 1:\n",
    "        if velocity.shape[0] < velocity.shape[1]:\n",
    "            snapshot_matrix = velocity.T  # Transpose to get (n_points, n_snapshots)\n",
    "        else:\n",
    "            snapshot_matrix = velocity  # Already (n_points, n_snapshots)\n",
    "    else:\n",
    "        snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "        for i in range(n_snapshots):\n",
    "            snapshot_matrix[:, i] = velocity[i].flatten()\n",
    "\n",
    "print(f\"Created snapshot matrix with shape: {snapshot_matrix.shape}\")\n",
    "\n",
    "# Compute mean flow\n",
    "mean_flow = np.mean(snapshot_matrix, axis=1, keepdims=True)\n",
    "print(f\"Mean flow shape: {mean_flow.shape}\")\n",
    "\n",
    "# Subtract mean (centering the data)\n",
    "if params[\"preprocessing_options\"][\"mean_subtraction\"]:\n",
    "    fluctuations = snapshot_matrix - mean_flow\n",
    "    print(\"Mean subtraction applied\")\n",
    "else:\n",
    "    fluctuations = snapshot_matrix\n",
    "    print(\"Mean subtraction skipped\")\n",
    "\n",
    "# Normalize the data if needed\n",
    "if params[\"preprocessing_options\"][\"normalization\"]:\n",
    "    # Compute the Frobenius norm of the fluctuation matrix\n",
    "    frob_norm = np.linalg.norm(fluctuations)\n",
    "    # Normalize\n",
    "    fluctuations = fluctuations / frob_norm\n",
    "    print(f\"Normalization applied (Frobenius norm: {frob_norm:.4f})\")\n",
    "    \n",
    "    # Save the normalization factor for later use\n",
    "    params[\"preprocessing_options\"][\"normalization_factor\"] = float(frob_norm)\n",
    "else:\n",
    "    print(\"Normalization skipped\")\n",
    "\n",
    "# Visualize the mean flow\n",
    "if structured_grid:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if n_dims == 1:\n",
    "        # Scalar field\n",
    "        plt.imshow(mean_flow.reshape(grid_shape), cmap='viridis')\n",
    "        plt.colorbar(label='Mean Velocity')\n",
    "    else:\n",
    "        # Vector field - plot magnitude\n",
    "        mean_reshaped = mean_flow.reshape(grid_shape[0], grid_shape[1], n_dims)\n",
    "        mean_magnitude = np.sqrt(np.sum(mean_reshaped**2, axis=2))\n",
    "        plt.imshow(mean_magnitude, cmap='viridis')\n",
    "        plt.colorbar(label='Mean Velocity Magnitude')\n",
    "        \n",
    "    plt.title('Mean Flow')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization to local file\n",
    "    mean_flow_img = os.path.join(temp_dir, 'mean_flow.png')\n",
    "    plt.savefig(mean_flow_img)\n",
    "    \n",
    "    # Upload to MinIO\n",
    "    # Make sure visualizations directory exists in MinIO\n",
    "    try:\n",
    "        s3.put_object(Bucket=MINIO_BUCKET, Key=f\"{MINIO_OUTPUT_PREFIX}/visualizations/\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create visualizations directory: {str(e)}\")\n",
    "    \n",
    "    s3.upload_file(\n",
    "        mean_flow_img, \n",
    "        MINIO_BUCKET, \n",
    "        f\"{MINIO_OUTPUT_PREFIX}/visualizations/mean_flow.png\"\n",
    "    )\n",
    "    print(f\"Mean flow visualization uploaded to {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/visualizations/mean_flow.png\")\n",
    "\n",
    "# Visualize the first fluctuation mode\n",
    "if structured_grid:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if n_dims == 1:\n",
    "        # Scalar field\n",
    "        plt.imshow(fluctuations[:, 0].reshape(grid_shape), cmap='RdBu_r')\n",
    "        plt.colorbar(label='Velocity Fluctuation')\n",
    "    else:\n",
    "        # Vector field - plot magnitude\n",
    "        fluct_reshaped = fluctuations[:, 0].reshape(grid_shape[0], grid_shape[1], n_dims)\n",
    "        fluct_magnitude = np.sqrt(np.sum(fluct_reshaped**2, axis=2))\n",
    "        plt.imshow(fluct_magnitude, cmap='viridis')\n",
    "        plt.colorbar(label='Velocity Fluctuation Magnitude')\n",
    "        \n",
    "    plt.title('First Fluctuation Snapshot')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization to local file\n",
    "    fluct_img = os.path.join(temp_dir, 'first_fluctuation.png')\n",
    "    plt.savefig(fluct_img)\n",
    "    \n",
    "    # Upload to MinIO\n",
    "    s3.upload_file(\n",
    "        fluct_img, \n",
    "        MINIO_BUCKET, \n",
    "        f\"{MINIO_OUTPUT_PREFIX}/visualizations/first_fluctuation.png\"\n",
    "    )\n",
    "    print(f\"First fluctuation visualization uploaded to {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/visualizations/first_fluctuation.png\")\n",
    "\n",
    "# Make sure preprocessed directory exists in MinIO\n",
    "try:\n",
    "    s3.put_object(Bucket=MINIO_BUCKET, Key=f\"{MINIO_OUTPUT_PREFIX}/preprocessed/\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create preprocessed directory: {str(e)}\")\n",
    "\n",
    "# Save preprocessed data to local files then upload to MinIO\n",
    "# Save snapshot matrix\n",
    "snapshot_path = os.path.join(temp_dir, 'snapshot_matrix.npy')\n",
    "np.save(snapshot_path, snapshot_matrix)\n",
    "s3.upload_file(\n",
    "    snapshot_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessed/snapshot_matrix.npy\"\n",
    ")\n",
    "\n",
    "# Save mean flow\n",
    "mean_flow_path = os.path.join(temp_dir, 'mean_flow.npy')\n",
    "np.save(mean_flow_path, mean_flow)\n",
    "s3.upload_file(\n",
    "    mean_flow_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessed/mean_flow.npy\"\n",
    ")\n",
    "\n",
    "# Save fluctuations\n",
    "fluctuations_path = os.path.join(temp_dir, 'fluctuations.npy')\n",
    "np.save(fluctuations_path, fluctuations)\n",
    "s3.upload_file(\n",
    "    fluctuations_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessed/fluctuations.npy\"\n",
    ")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"n_snapshots\": n_snapshots,\n",
    "    \"n_points\": n_points,\n",
    "    \"n_dims\": n_dims,\n",
    "    \"structured_grid\": structured_grid,\n",
    "    \"preprocessing_params\": params[\"preprocessing_options\"]\n",
    "}\n",
    "\n",
    "if structured_grid:\n",
    "    metadata[\"grid_shape\"] = grid_shape\n",
    "\n",
    "metadata_path = os.path.join(temp_dir, 'metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "s3.upload_file(\n",
    "    metadata_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessed/metadata.json\"\n",
    ")\n",
    "\n",
    "# Update and save parameters for the next step\n",
    "params[\"preprocessing_metadata\"] = metadata\n",
    "params_path = os.path.join(temp_dir, 'params.json')\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "s3.upload_file(\n",
    "    params_path, \n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    ")\n",
    "\n",
    "# Upload completion marker\n",
    "with open(os.path.join(temp_dir, 'preprocessing_completed.txt'), 'w') as f:\n",
    "    f.write(\"Preprocessing completed successfully\")\n",
    "\n",
    "s3.upload_file(\n",
    "    os.path.join(temp_dir, 'preprocessing_completed.txt'),\n",
    "    MINIO_BUCKET, \n",
    "    f\"{MINIO_OUTPUT_PREFIX}/preprocessing_completed.txt\"\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing completed successfully!\")\n",
    "print(f\"Preprocessed data uploaded to: {MINIO_BUCKET}/{MINIO_OUTPUT_PREFIX}/preprocessed/\")\n",
    "\n",
    "# List objects in the bucket to verify uploads\n",
    "print(\"\\nVerifying uploaded files:\")\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=f\"{MINIO_OUTPUT_PREFIX}/preprocessed/\")\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "else:\n",
    "    print(\"No objects found in preprocessed location\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
