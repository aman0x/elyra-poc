{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e748f11-2aa3-4dc5-bdbb-e88fbb90db34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing MarkupSafe==1.1.1...\n",
      "Requirement already satisfied: MarkupSafe==1.1.1 in /opt/conda/lib/python3.9/site-packages (1.1.1)\n",
      "Successfully installed MarkupSafe==1.1.1\n",
      "Installing Jinja2==2.11.3...\n",
      "Requirement already satisfied: Jinja2==2.11.3 in /opt/conda/lib/python3.9/site-packages (2.11.3)\n",
      "Successfully installed Jinja2==2.11.3\n",
      "Installing boto3...\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.9/site-packages (1.38.21)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.21 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.38.21)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /opt/conda/lib/python3.9/site-packages (from boto3) (0.12.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.39.0,>=1.38.21->boto3) (1.26.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.21->boto3) (1.16.0)\n",
      "Successfully installed boto3\n",
      "Installing h5py...\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.9/site-packages (3.13.0)\n",
      "Successfully installed h5py\n",
      "Installing matplotlib...\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.7.1)\n",
      "Successfully installed matplotlib\n",
      "Dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies with specific versions to avoid conflicts\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def install_package(package, version=None):\n",
    "    \"\"\"Install a package with specific version if needed.\"\"\"\n",
    "    if version:\n",
    "        package_with_version = f\"{package}=={version}\"\n",
    "    else:\n",
    "        package_with_version = package\n",
    "    \n",
    "    print(f\"Installing {package_with_version}...\")\n",
    "    if package in [\"boto3\"]:  # Allow boto3 to install its dependencies\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_with_version])\n",
    "    else:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                              package_with_version, \"--no-deps\"])\n",
    "    print(f\"Successfully installed {package_with_version}\")\n",
    "\n",
    "# Install specific versions to resolve conflicts\n",
    "install_package(\"MarkupSafe\", \"1.1.1\")  # Version compatible with cookiecutter\n",
    "install_package(\"Jinja2\", \"2.11.3\")     # Version compatible with cookiecutter\n",
    "\n",
    "# Install boto3 WITH its dependencies (including botocore)\n",
    "install_package(\"boto3\")                # For S3/MinIO operations\n",
    "install_package(\"h5py\")                # For HDF5 file operations\n",
    "install_package(\"matplotlib\")          # For plotting\n",
    "\n",
    "# Force reload the modules if they were already imported\n",
    "if \"markupsafe\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"markupsafe\"])\n",
    "if \"jinja2\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"jinja2\"])\n",
    "\n",
    "print(\"Dependencies installed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324ffc8-2fb4-46e6-abd7-7de10e54aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1_preprocess_data.ipynb\n",
    "#\n",
    "# This notebook preprocesses the Cylinder Flow Dataset:\n",
    "# - Loads the data from MinIO\n",
    "# - Extracts velocity fields\n",
    "# - Creates the snapshot matrix\n",
    "# - Performs mean subtraction\n",
    "# - Normalizes the data if needed\n",
    "# - Uploads processed data back to MinIO\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import tempfile\n",
    "import io\n",
    "\n",
    "# Create a temporary local directory for processing\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Connect to MinIO\n",
    "print(\"Connecting to MinIO...\")\n",
    "s3_endpoint = os.environ.get('S3_ENDPOINT', 'http://minio.minio-system.svc.cluster.local:9000')\n",
    "\n",
    "# Fix the endpoint URL if the protocol is missing\n",
    "if s3_endpoint and not s3_endpoint.startswith(('http://', 'https://')):\n",
    "    s3_endpoint = f\"http://{s3_endpoint}\"\n",
    "    print(f\"Adding http:// prefix to endpoint: {s3_endpoint}\")\n",
    "\n",
    "s3_access_key = os.environ.get('AWS_ACCESS_KEY_ID', 'minio')\n",
    "s3_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY', 'minio123')\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  endpoint_url=s3_endpoint,\n",
    "                  aws_access_key_id=s3_access_key,\n",
    "                  aws_secret_access_key=s3_secret_key,\n",
    "                  config=Config(signature_version='s3v4'))\n",
    "\n",
    "# Load parameters from previous step\n",
    "MINIO_BUCKET = 'rom-data'\n",
    "MINIO_OUTPUT_PREFIX = 'rom-pipeline/outputs'\n",
    "\n",
    "# Download parameters file from MinIO\n",
    "try:\n",
    "    params_key = f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "    params_path = os.path.join(temp_dir, 'params.json')\n",
    "    s3.download_file(MINIO_BUCKET, params_key, params_path)\n",
    "    \n",
    "    with open(params_path, 'r') as f:\n",
    "        params = json.load(f)\n",
    "        \n",
    "    print(f\"Loaded parameters: {params}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading parameters, using defaults: {str(e)}\")\n",
    "    params = {\n",
    "        \"dataset_name\": \"cylinder\",\n",
    "        \"minio_bucket\": MINIO_BUCKET,\n",
    "        \"minio_output_prefix\": MINIO_OUTPUT_PREFIX\n",
    "    }\n",
    "\n",
    "# Update parameters for this step\n",
    "params.update({\n",
    "    \"preprocessing_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"preprocessing_options\": {\n",
    "        \"mean_subtraction\": True,\n",
    "        \"normalization\": True\n",
    "    }\n",
    "})\n",
    "\n",
    "# Verify the previous step completed\n",
    "try:\n",
    "    marker_key = f\"{MINIO_OUTPUT_PREFIX}/data_download_completed.txt\"\n",
    "    s3.head_object(Bucket=MINIO_BUCKET, Key=marker_key)\n",
    "    print(\"Previous step (data fetching) completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Previous step completion marker not found: {str(e)}\")\n",
    "    print(\"Continuing anyway...\")\n",
    "\n",
    "# List objects in the data directory to check what's available\n",
    "print(\"\\nListing files in MinIO data directory:\")\n",
    "data_prefix = f\"{MINIO_OUTPUT_PREFIX}/data/\"\n",
    "response = s3.list_objects_v2(Bucket=MINIO_BUCKET, Prefix=data_prefix)\n",
    "\n",
    "if 'Contents' not in response or len(response.get('Contents', [])) == 0:\n",
    "    print(f\"Warning: No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "    print(\"Please ensure that data files have been uploaded.\")\n",
    "    raise FileNotFoundError(f\"No data files found in {MINIO_BUCKET}/{data_prefix}\")\n",
    "\n",
    "# Download data files from MinIO\n",
    "print(\"\\nDownloading data files from MinIO:\")\n",
    "for obj in response.get('Contents', []):\n",
    "    if obj['Key'].endswith(('.h5', '.hdf5')):\n",
    "        filename = os.path.basename(obj['Key'])\n",
    "        local_path = os.path.join(temp_dir, filename)\n",
    "        \n",
    "        print(f\"  Downloading {obj['Key']} to {local_path}\")\n",
    "        s3.download_file(MINIO_BUCKET, obj['Key'], local_path)\n",
    "\n",
    "# Find HDF5 files\n",
    "h5_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) \n",
    "            if f.endswith('.h5') or f.endswith('.hdf5')]\n",
    "\n",
    "if not h5_files:\n",
    "    raise FileNotFoundError(f\"No HDF5 files found in downloaded data\")\n",
    "\n",
    "print(f\"Found {len(h5_files)} HDF5 files: {[os.path.basename(f) for f in h5_files]}\")\n",
    "\n",
    "# Load velocity data from the first file\n",
    "h5_file = h5_files[0]\n",
    "print(f\"Loading data from {os.path.basename(h5_file)}\")\n",
    "\n",
    "try:\n",
    "    with h5py.File(h5_file, 'r') as f:\n",
    "        # First, explore the HDF5 file structure to find the velocity field\n",
    "        print(\"Exploring HDF5 file structure:\")\n",
    "        print(\"Top-level groups/datasets:\")\n",
    "        for key in f.keys():\n",
    "            if isinstance(f[key], h5py.Group):\n",
    "                print(f\"  Group: {key} (contains: {list(f[key].keys())})\")\n",
    "            elif isinstance(f[key], h5py.Dataset):\n",
    "                print(f\"  Dataset: {key} (shape: {f[key].shape}, dtype: {f[key].dtype})\")\n",
    "        \n",
    "        # Define possible velocity field names to check\n",
    "        velocity_keys = ['velocity', 'u', 'v', 'vel', 'flow', 'flowfield', 'vector']\n",
    "        \n",
    "        # Search for velocity data\n",
    "        velocity_key = None\n",
    "        \n",
    "        # Step 1: Check direct keys in root level\n",
    "        for key in velocity_keys:\n",
    "            if key in f:\n",
    "                velocity_key = key\n",
    "                print(f\"Found direct velocity key: {key}\")\n",
    "                break\n",
    "        \n",
    "        # Step 2: Look for keys containing velocity terms\n",
    "        if velocity_key is None:\n",
    "            for key in f.keys():\n",
    "                if any(vk in key.lower() for vk in velocity_keys):\n",
    "                    velocity_key = key\n",
    "                    print(f\"Found key containing velocity term: {key}\")\n",
    "                    break\n",
    "        \n",
    "        # Step 3: Search in groups\n",
    "        if velocity_key is None:\n",
    "            for key in f.keys():\n",
    "                if isinstance(f[key], h5py.Group):\n",
    "                    for subkey in f[key].keys():\n",
    "                        if any(vk == subkey.lower() or vk in subkey.lower() for vk in velocity_keys):\n",
    "                            velocity_key = f\"{key}/{subkey}\"\n",
    "                            print(f\"Found velocity data in group: {velocity_key}\")\n",
    "                            break\n",
    "        \n",
    "        # Step 4: Look for large datasets (likely to be the flow field)\n",
    "        if velocity_key is None:\n",
    "            largest_dataset = None\n",
    "            largest_size = 0\n",
    "            \n",
    "            def find_largest_dataset(name, obj):\n",
    "                global largest_dataset, largest_size\n",
    "                if isinstance(obj, h5py.Dataset):\n",
    "                    # Skip small datasets (likely metadata)\n",
    "                    if obj.size > 1000 and obj.size > largest_size:\n",
    "                        largest_size = obj.size\n",
    "                        largest_dataset = name\n",
    "            \n",
    "            # Traverse the entire file\n",
    "            f.visititems(find_largest_dataset)\n",
    "            \n",
    "            if largest_dataset:\n",
    "                velocity_key = largest_dataset\n",
    "                print(f\"Selected largest dataset as velocity data: {velocity_key} (size: {largest_size})\")\n",
    "        \n",
    "        if velocity_key is None:\n",
    "            raise KeyError(\"Could not find velocity data in the file\")\n",
    "            \n",
    "        print(f\"Loading velocity data from '{velocity_key}'\")\n",
    "        velocity = f[velocity_key][:]\n",
    "        \n",
    "        # Load any coordinates or metadata if available\n",
    "        x_coords = None\n",
    "        y_coords = None\n",
    "        time_data = None\n",
    "        \n",
    "        # Look for common coordinate names\n",
    "        for coord_name in ['x', 'y', 'z', 'X', 'Y', 'Z', 'coord', 'coords', 'coordinates', 'grid', 'time', 't']:\n",
    "            if coord_name in f:\n",
    "                if coord_name.lower() in ['x', 'X']:\n",
    "                    x_coords = f[coord_name][:]\n",
    "                    print(f\"Found x coordinates: shape {x_coords.shape}\")\n",
    "                elif coord_name.lower() in ['y', 'Y']:\n",
    "                    y_coords = f[coord_name][:]\n",
    "                    print(f\"Found y coordinates: shape {y_coords.shape}\")\n",
    "                elif coord_name.lower() in ['time', 't']:\n",
    "                    time_data = f[coord_name][:]\n",
    "                    print(f\"Found time data: shape {time_data.shape}\")\n",
    "                else:\n",
    "                    print(f\"Found other coordinate data '{coord_name}': shape {f[coord_name].shape}\")\n",
    "        \n",
    "        print(f\"Velocity data shape: {velocity.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Determine data dimensions\n",
    "if len(velocity.shape) == 2:  # (time, points) or (points, time)\n",
    "    if velocity.shape[0] < velocity.shape[1]:  # Likely (time, points)\n",
    "        n_snapshots = velocity.shape[0]\n",
    "        n_points = velocity.shape[1]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (points, time)\n",
    "        n_snapshots = velocity.shape[1]\n",
    "        n_points = velocity.shape[0]\n",
    "        n_dims = 1\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "        \n",
    "elif len(velocity.shape) == 3:  # (time, x, y) or (time, points, dims)\n",
    "    if velocity.shape[1] > velocity.shape[2]:  # Likely (time, points, dims)\n",
    "        n_snapshots = velocity.shape[0]\n",
    "        n_points = velocity.shape[1]\n",
    "        n_dims = velocity.shape[2]\n",
    "        structured_grid = False\n",
    "        print(f\"Detected unstructured data: {n_snapshots} snapshots, {n_points} points, {n_dims} dimensions\")\n",
    "    else:  # Likely (time, x, y) for a scalar field\n",
    "        n_snapshots = velocity.shape[0]\n",
    "        grid_shape = (velocity.shape[1], velocity.shape[2])\n",
    "        n_points = grid_shape[0] * grid_shape[1]\n",
    "        structured_grid = True\n",
    "        n_dims = 1\n",
    "        print(f\"Detected structured scalar data: {n_snapshots} snapshots, grid shape {grid_shape}\")\n",
    "        \n",
    "elif len(velocity.shape) == 4:  # (time, x, y, components)\n",
    "    n_snapshots = velocity.shape[0]\n",
    "    grid_shape = (velocity.shape[1], velocity.shape[2])\n",
    "    n_points = grid_shape[0] * grid_shape[1]\n",
    "    n_dims = velocity.shape[3]\n",
    "    structured_grid = True\n",
    "    print(f\"Detected structured vector data: {n_snapshots} snapshots, grid shape {grid_shape}, {n_dims} components\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unexpected velocity data shape: {velocity.shape}\")\n",
    "\n",
    "# Create snapshot matrix - reshape data into a 2D matrix\n",
    "# For POD, we need a matrix of shape (n_points*n_dims, n_snapshots)\n",
    "if structured_grid:\n",
    "    if n_dims == 1:\n",
    "        # For scalar field\n",
    "        snapshot_matrix = np.zeros((n_points, n_snapshots))\n",
    "        for i in range(n_snapshots):\n",
    "            snapshot_matrix[:, i] = velocity[i].reshape(n_points)\n",
    "    else:\n",
    "        # For vector field\n",
    "        snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "        for i in range(n_snapshots):\n",
    "            # Reshape and stack components\n",
    "            reshaped = velocity[i].reshape(n_points, n_dims)\n",
    "            snapshot_matrix[:, i] = reshaped.flatten()\n",
    "else:\n",
    "    # Already in the right format for unstructured grid\n",
    "    if n_dims == 1:\n",
    "        if velocity.shape[0] < velocity.shape[1]:\n",
    "            snapshot_matrix = velocity.T  # Transpose to get (n_points, n_snapshots)\n",
    "        else:\n",
    "            snapshot_matrix = velocity  # Already (n_points, n_snapshots)\n",
    "    else:\n",
    "        snapshot_matrix = np.zeros((n_points * n_dims, n_snapshots))\n",
    "        for i in range(n_snapshots):\n",
    "            snapshot_matrix[:, i] = velocity[i].flatten()\n",
    "\n",
    "print(f\"Created snapshot matrix with shape: {snapshot_matrix.shape}\")\n",
    "\n",
    "# Compute mean flow\n",
    "mean_flow = np.mean(snapshot_matrix, axis=1, keepdims=True)\n",
    "print(f\"Mean flow shape: {mean_flow.shape}\")\n",
    "\n",
    "# Subtract mean (centering the data)\n",
    "if params[\"preprocessing_options\"][\"mean_subtraction\"]:\n",
    "    fluctuations = snapshot_matrix - mean_flow\n",
    "    print(\"Mean subtraction applied\")\n",
    "else:\n",
    "    fluctuations = snapshot_matrix\n",
    "    print(\"Mean subtraction skipped\")\n",
    "\n",
    "# Normalize the data if needed\n",
    "if params[\"preprocessing_options\"][\"normalization\"]:\n",
    "    # Compute the Frobenius norm of the fluctuation matrix\n",
    "    frob_norm = np.linalg.norm(fluctuations)\n",
    "    # Normalize\n",
    "    fluctuations = fluctuations / frob_norm\n",
    "    print(f\"Normalization applied (Frobenius norm: {frob_norm:.4f})\")\n",
    "    \n",
    "    # Save the normalization factor for later use\n",
    "    params[\"preprocessing_options\"][\"normalization_factor\"] = float(frob_norm)\n",
    "else:\n",
    "    print(\"Normalization skipped\")\n",
    "\n",
    "# Visualize the mean flow\n",
    "if structured_grid:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if n_dims == 1:\n",
    "        # Scalar field\n",
    "        plt.imshow(mean_flow.reshape(grid_shape), cmap='viridis')\n",
    "        plt.colorbar(label='Mean Velocity')\n",
    "    else:\n",
    "        # Vector field - plot magnitude\n",
    "        mean_reshaped = mean_flow.reshape(grid_shape[0], grid_shape[1], n_dims)\n",
    "        mean_magnitude = np.sqrt(np.sum(mean_reshaped**2, axis=2))\n",
    "        plt.imshow(mean_magnitude, cmap='viridis')\n",
    "        plt.colorbar(label='Mean Velocity Magnitude')\n",
    "        \n",
    "    plt.title('Mean Flow')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization to local file\n",
    "    mean_flow_img = os.path.join(temp_dir, 'mean_flow.png')\n",
    "    plt.savefig(mean_flow_img)\n",
    "    \n",
    "    # Upload visualization to MinIO\n",
    "    vis_key = f\"{MINIO_OUTPUT_PREFIX}/visualizations/mean_flow.png\"\n",
    "    print(f\"Uploading mean flow visualization to {MINIO_BUCKET}/{vis_key}\")\n",
    "    s3.upload_file(mean_flow_img, MINIO_BUCKET, vis_key)\n",
    "\n",
    "# Visualize a few snapshots\n",
    "if structured_grid and n_snapshots > 0:\n",
    "    # Create a directory for snapshot visualizations\n",
    "    vis_dir = os.path.join(temp_dir, 'snapshots')\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    \n",
    "    # Visualize a few snapshots (original and fluctuations)\n",
    "    num_vis = min(5, n_snapshots)  # Visualize up to 5 snapshots\n",
    "    indices = np.linspace(0, n_snapshots-1, num_vis, dtype=int)\n",
    "    \n",
    "    for idx in indices:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Original snapshot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if n_dims == 1:\n",
    "            # Scalar field\n",
    "            plt.imshow(snapshot_matrix[:, idx].reshape(grid_shape), cmap='viridis')\n",
    "            plt.colorbar(label='Velocity')\n",
    "        else:\n",
    "            # Vector field - plot magnitude\n",
    "            snapshot_reshaped = snapshot_matrix[:, idx].reshape(grid_shape[0], grid_shape[1], n_dims)\n",
    "            snapshot_magnitude = np.sqrt(np.sum(snapshot_reshaped**2, axis=2))\n",
    "            plt.imshow(snapshot_magnitude, cmap='viridis')\n",
    "            plt.colorbar(label='Velocity Magnitude')\n",
    "            \n",
    "        plt.title(f'Original Snapshot {idx}')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        \n",
    "        # Fluctuation\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if n_dims == 1:\n",
    "            # Scalar field\n",
    "            plt.imshow(fluctuations[:, idx].reshape(grid_shape), cmap='coolwarm')\n",
    "            plt.colorbar(label='Fluctuation')\n",
    "        else:\n",
    "            # Vector field - plot magnitude\n",
    "            fluct_reshaped = fluctuations[:, idx].reshape(grid_shape[0], grid_shape[1], n_dims)\n",
    "            fluct_magnitude = np.sqrt(np.sum(fluct_reshaped**2, axis=2))\n",
    "            plt.imshow(fluct_magnitude, cmap='coolwarm')\n",
    "            plt.colorbar(label='Fluctuation Magnitude')\n",
    "            \n",
    "        plt.title(f'Fluctuation Snapshot {idx}')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save visualization to local file\n",
    "        snapshot_img = os.path.join(vis_dir, f'snapshot_{idx}.png')\n",
    "        plt.savefig(snapshot_img)\n",
    "        plt.close()\n",
    "        \n",
    "        # Upload visualization to MinIO\n",
    "        vis_key = f\"{MINIO_OUTPUT_PREFIX}/visualizations/snapshot_{idx}.png\"\n",
    "        print(f\"Uploading snapshot visualization to {MINIO_BUCKET}/{vis_key}\")\n",
    "        s3.upload_file(snapshot_img, MINIO_BUCKET, vis_key)\n",
    "\n",
    "# Save the processed data\n",
    "processed_data = {\n",
    "    'snapshot_matrix': snapshot_matrix,\n",
    "    'mean_flow': mean_flow,\n",
    "    'fluctuations': fluctuations,\n",
    "    'n_snapshots': n_snapshots,\n",
    "    'n_points': n_points,\n",
    "    'n_dims': n_dims,\n",
    "    'structured_grid': structured_grid\n",
    "}\n",
    "\n",
    "if structured_grid:\n",
    "    processed_data['grid_shape'] = grid_shape\n",
    "\n",
    "if x_coords is not None:\n",
    "    processed_data['x_coords'] = x_coords\n",
    "if y_coords is not None:\n",
    "    processed_data['y_coords'] = y_coords\n",
    "if time_data is not None:\n",
    "    processed_data['time_data'] = time_data\n",
    "\n",
    "# Save processed data to NPZ file\n",
    "processed_data_file = os.path.join(temp_dir, 'processed_data.npz')\n",
    "print(f\"Saving processed data to {processed_data_file}\")\n",
    "np.savez_compressed(processed_data_file, **processed_data)\n",
    "\n",
    "# Upload processed data to MinIO\n",
    "processed_data_key = f\"{MINIO_OUTPUT_PREFIX}/processed_data.npz\"\n",
    "print(f\"Uploading processed data to {MINIO_BUCKET}/{processed_data_key}\")\n",
    "s3.upload_file(processed_data_file, MINIO_BUCKET, processed_data_key)\n",
    "\n",
    "# Save parameters\n",
    "params_file = os.path.join(temp_dir, 'params.json')\n",
    "with open(params_file, 'w') as f:\n",
    "    json.dump(params, f, indent=2)\n",
    "\n",
    "# Upload parameters to MinIO\n",
    "params_key = f\"{MINIO_OUTPUT_PREFIX}/params.json\"\n",
    "print(f\"Uploading parameters to {MINIO_BUCKET}/{params_key}\")\n",
    "s3.upload_file(params_file, MINIO_BUCKET, params_key)\n",
    "\n",
    "# Create a completion marker\n",
    "completion_marker = os.path.join(temp_dir, 'preprocessing_completed.txt')\n",
    "with open(completion_marker, 'w') as f:\n",
    "    f.write(f\"Preprocessing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Processed {n_snapshots} snapshots with {n_points} spatial points and {n_dims} dimensions\\n\")\n",
    "\n",
    "# Upload completion marker to MinIO\n",
    "marker_key = f\"{MINIO_OUTPUT_PREFIX}/preprocessing_completed.txt\"\n",
    "print(f\"Uploading completion marker to {MINIO_BUCKET}/{marker_key}\")\n",
    "s3.upload_file(completion_marker, MINIO_BUCKET, marker_key)\n",
    "\n",
    "print(\"\\nPreprocessing completed successfully!\")\n",
    "print(f\"Processed {n_snapshots} snapshots with {n_points} spatial points and {n_dims} dimensions\")\n",
    "print(f\"Data saved to {MINIO_BUCKET}/{processed_data_key}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
