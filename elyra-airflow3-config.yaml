apiVersion: v1
kind: ConfigMap
metadata:
  name: elyra-airflow3-config
data:
  airflow_template.jinja2: |
    # Generated for Airflow 3.0 compatibility
    import pendulum
    from airflow import DAG
    {% for import_statement in imports %}
    {{ import_statement }}
    {% endfor %}

    args = {
        "project_id": "{{ pipeline_name }}",
    }

    dag = DAG(
        "{{ pipeline_name }}",
        default_args=args,
        schedule="{{ schedule }}",
        start_date=pendulum.datetime(2025, 5, 19, tz="UTC"),
        description="""
    Created with Elyra {{ elyra_version }} pipeline editor using `{{ pipeline_path }}`.
        """,
        is_paused_upon_creation=False,
    )

    {% for node in nodes %}

    # Operator source: {{ node.filename }}

    {{ node.id }} = {{ node.component_source }}(
        name="{{ node.name }}",
        namespace="airflow-new",
        image="{{ node.image }}",
    {% if node.env_vars %}
        env_vars={{ node.env_vars }},
    {% endif %}
    {% if node.include_workflow_name %}
        env_vars={
            "ELYRA_RUNTIME_ENV": "airflow-new",
            "AWS_ACCESS_KEY_ID": "minio",
            "AWS_SECRET_ACCESS_KEY": "minio123",
            "ELYRA_ENABLE_PIPELINE_INFO": "True",
            "ELYRA_RUN_NAME": "{{ pipeline_name }}-{{ '{{' }} ts_nodash {{ '}}' }}",
        },
    {% endif %}
    {% if 'KubernetesPodOperator' in node.component_source %}
        cmds=["sh", "-c"],
        arguments=[
            "{{ node.command_line }}"
        ],
        task_id="{{ node.task_id }}",
        volumes=[],
        volume_mounts=[],
        secrets=[],
        annotations={},
        labels={},
        tolerations=[],
        in_cluster=True,
        config_file=None,
    {% else %}
        task_id="{{ node.task_id }}",
    {% endif %}
        dag=dag,
    )

    {% for parent in node.parent_nodes %}
    {{ node.id }} << {{ parent.id }}
    {% endfor %}
    {% endfor %}
  compatibility_checker.py: |
    #!/usr/bin/env python3
    """
    Tool to check DAG compatibility with Airflow 3.0 and apply fixes.
    This script can be run directly on DAG files or directories.
    """
    
    import argparse
    import os
    import re
    import sys
    from datetime import datetime
    
    def fix_file(file_path, namespace="airflow-new", dry_run=False):
        """Analyze and fix an Airflow DAG file for 3.0 compatibility."""
        with open(file_path, 'r') as f:
            content = f.read()
            
        # Skip if already processed
        if "# PROCESSED_BY_SCRIPT" in content:
            print(f"Skipping {file_path}: already processed")
            return False
            
        original = content
        
        # Fix imports
        content = re.sub(
            r'from\s+airflow\.contrib\.operators\.kubernetes_pod_operator\s+import\s+KubernetesPodOperator',
            'from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator',
            content
        )
        
        content = re.sub(
            r'from\s+airflow\.contrib\.kubernetes\.volume_mount\s+import\s+VolumeMount',
            'from kubernetes.client.models import V1VolumeMount as VolumeMount',
            content
        )
        
        content = re.sub(
            r'from\s+airflow\.contrib\.kubernetes\.volume\s+import\s+Volume',
            'from kubernetes.client.models import V1Volume as Volume',
            content
        )
        
        content = re.sub(
            r'from\s+airflow\.kubernetes\.secret\s+import\s+Secret',
            'from airflow.providers.cncf.kubernetes.secret import Secret',
            content
        )
        
        # Fix days_ago
        content = re.sub(
            r'from\s+airflow\.utils\.dates\s+import\s+days_ago',
            'import pendulum',
            content
        )
        
        # Replace days_ago(X) with pendulum datetime
        content = re.sub(
            r'start_date\s*=\s*days_ago\((\d+)\)',
            r'start_date=pendulum.datetime(2025, 5, 19, tz="UTC")',
            content
        )
        
        # Fix schedule_interval to schedule
        content = re.sub(
            r'schedule_interval\s*=',
            'schedule=',
            content
        )
        
        # Fix namespace
        content = re.sub(
            r'namespace\s*=\s*"airflow"',
            f'namespace="{namespace}"',
            content
        )
        
        # Fix config_file
        content = re.sub(
            r'config_file\s*=\s*"None"',
            'config_file=None',
            content
        )
        
        # Fix ELYRA_RUNTIME_ENV
        content = re.sub(
            r'"ELYRA_RUNTIME_ENV"\s*:\s*"airflow"',
            f'"ELYRA_RUNTIME_ENV": "{namespace}"',
            content
        )
        
        # Add header if changes were made
        if content != original:
            content = f"# PROCESSED_BY_SCRIPT\n# Modified for Airflow 3.0 compatibility on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{content}"
            
            if not dry_run:
                # Create backup
                with open(f"{file_path}.bak", 'w') as f:
                    f.write(original)
                
                # Write changes
                with open(file_path, 'w') as f:
                    f.write(content)
                
                print(f"Updated {file_path} for Airflow 3.0 compatibility")
            else:
                print(f"Would update {file_path} (dry run)")
            
            return True
        
        print(f"No changes needed for {file_path}")
        return False
    
    def process_directory(directory, namespace="airflow-new", dry_run=False, recursive=True):
        """Process all .py files in a directory."""
        count = 0
        
        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    if fix_file(file_path, namespace, dry_run):
                        count += 1
            
            if not recursive:
                break
        
        return count
    
    def main():
        parser = argparse.ArgumentParser(description='Check and fix Airflow DAG compatibility with Airflow 3.0')
        parser.add_argument('path', help='Path to DAG file or directory')
        parser.add_argument('--namespace', default='airflow-new', help='Kubernetes namespace to use')
        parser.add_argument('--dry-run', action='store_true', help='Show changes without applying them')
        parser.add_argument('--recursive', action='store_true', help='Process directories recursively')
        
        args = parser.parse_args()
        
        if os.path.isfile(args.path):
            fix_file(args.path, args.namespace, args.dry_run)
        elif os.path.isdir(args.path):
            count = process_directory(args.path, args.namespace, args.dry_run, args.recursive)
            print(f"Processed {count} files in {args.path}")
        else:
            print(f"Error: {args.path} is not a valid file or directory")
            return 1
        
        return 0
    
    if __name__ == '__main__':
        sys.exit(main())